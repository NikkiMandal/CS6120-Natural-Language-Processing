{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Classification, some n-gram math and tf-idf\n",
    "\n",
    "Due date: Jan 29th, 2024, 11:59 PM\n",
    "\n",
    "Agenda\n",
    "------\n",
    "+ Detecting the end of a sentence\n",
    "    - Rule-based classifier\n",
    "+ Detecting the sentiment of a sentence\n",
    "    - Rule-based classifier (counting words)\n",
    "    - Measuring Accuracy, Precision, Recall (evaluating a classifier)\n",
    "+ N-gram Math \n",
    "+ tf-idf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification can take several forms. Here are some vocabulary terms to get you started:\n",
    "\n",
    "- __classifier__: a model that takes data (text, in NLP) as input and outputs a category\n",
    "- __binary classification__: a model that takes input and outputs *one of two* categories (e.g. \"positive\" or \"negative\")\n",
    "- __multinomial classification__: a model that takes input and outputs *one of many* categories (e.g. \"positive\", \"neutral\" or \"negative\" or a language model that chooses one token from the entire vocabulary)\n",
    "\n",
    "\n",
    "- __rule-based classifier__: a classifier that functions based on rules that humans come up with (e.g. \"the end of a sentence is when there is a \".\" \")\n",
    "- __statistical classifier__: a classifier that functions based on counts (statistics) that it has gathered or based on running an algorithm to automatically train parameters on a given data set. \n",
    "    \n",
    "In this lab, you'll be building rule-based classifiers and evaluating them. We'll learn about our first statistical classifier next lecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0: Who is in your group?\n",
    "\n",
    "__WRITE YOUR NAMES HERE__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Tokenizer\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(s: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenize a string based on whitespace\n",
    "    Parameters:\n",
    "        s - string piece of text\n",
    "    returns a list of strings from the text. \n",
    "    Each item is an individual linguistic unit. \n",
    "    \"\"\"\n",
    "    # TODO: fill in this function\n",
    "    # you may find using the str.split() functions\n",
    "    # helpful\n",
    "    # you may make whatever decisions make sense to you/your \n",
    "    # group about how to tokenize.\n",
    "    # don't spend longer than ~5 minutes implementing this function.\n",
    "    return s.split()\n",
    "\n",
    "test_string = \"fill me in with whatever you want!\"\n",
    "tokenized = tokenize(test_string)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What decisions does your tokenizer make about what should/should not be a token? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the text of moby dick (ensure the txt file is in the same directory as this notebook)\n",
    "# if you do not already have - link to download text http://www.gutenberg.org/files/2701/2701-0.txt \n",
    "# right click and 'save as' into the directory this notebook is located as 'moby_dick.txt'\n",
    "moby = open('mobydick.txt', \"r\", encoding='utf-8')\n",
    "\n",
    "print(moby.readline()) # first line is blank\n",
    "print(moby.readline()) # second line just to see if its correct\n",
    "moby.close()\n",
    "\n",
    "# now read in the full contents\n",
    "moby = open('mobydick.txt', \"r\", encoding='utf-8')\n",
    "contents = moby.read()\n",
    "moby.close()\n",
    "\n",
    "print(len(contents)) # how long is this string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call your tokenize function on the contents of moby dick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How many tokens are in *Moby Dick* when you use your `tokenize` function on its contents? __YOUR ANSWER HERE__\n",
    "3. How big is the __vocabulary__ of *Moby Dick* when you use your `tokenize` function on its contents? __YOUR ANSWER HERE__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Detecting the end of a sentence\n",
    "\n",
    "\n",
    "A classifier is, in essence, a function that takes some data $x$ and assigns some label $y$ to it. For a binary classifier, we can model this a function that takes a data point $x$ and returns either `True` or `False`.\n",
    "\n",
    "Later in this class we'll learn about how to build classifiers that automatically learn how to do this, but we'll start where NLP startedâ€”writing some rule-based classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence_end(text: str, target_index: int) -> bool: \n",
    "    \"\"\"\n",
    "    Classify whether or not a *location* is the end of a sentence within\n",
    "    a given text\n",
    "    Parameters:\n",
    "        text - string piece of text\n",
    "        target_index - int candidate location\n",
    "    returns true if the target index is the end of a sentence. \n",
    "    False otherwise. \n",
    "    \"\"\"\n",
    "    # TODO: write a simple, rule-based classifier that\n",
    "    # decides whether or not a specific location is the \n",
    "    # end of a sentence\n",
    "    pass \n",
    "\n",
    "# look at the code in the cell below to see example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example text\n",
    "# feel free to go through different examples\n",
    "\n",
    "# This is the given example text\n",
    "\"\"\"Stocks were up as advancing issues outpaced declining issues \n",
    "          on the NYSE by 1.5 to 1. Large- and small-cap stocks were both strong, \n",
    "          while the S.&P. 500 index gained 0.46% to finish at 2,457.59. Among \n",
    "          individual stocks, the two top percentage gainers in the S.&P. 500 \n",
    "          were Incyte Corporation and Gilead Sciences Inc.\"\"\"\n",
    "\n",
    "example = \"Stocks were up as advancing issues outpaced declining issues on the NYSE by 1.5 to 1. Large- and small-cap stocks were both strong, while the S.&P. 500 index gained 0.46% to finish at 2,457.59. Among individual stocks, the two top percentage gainers in the S.&P. 500 were Incyte Corporation and Gilead Sciences Inc.\"\n",
    "\n",
    "# this code will go through and\n",
    "# build up a string based on the sentence\n",
    "# decisions that your classifier comes up with\n",
    "# it will put \"****\" between the sentences\n",
    "# you do not need to modify any code here\n",
    "so_far = \"\"\n",
    "for index in range(len(example)):\n",
    "    # see how the classify_sentence_end function is called!\n",
    "    result = classify_sentence_end(example, index)\n",
    "    so_far += example[index]\n",
    "    if result:\n",
    "        print(so_far)\n",
    "        print(\"****\")\n",
    "        so_far = \"\"\n",
    "        \n",
    "print(so_far)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many sentences are detected using your end of sentence classifier? **YOUR ANSWER HERE**\n",
    "2. Where did your end of sentence classifier make a mistake? **YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: install `nltk`\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you finish the first two tasks, work on making sure that you have `nltk` downloaded and accessible to your jupyter notebooks. While you will not be allowed to use `nltk` for most of your homework, we will use it frequently in class to demonstrate tools. \n",
    "\n",
    "[`nltk`](https://www.nltk.org/) (natural language toolkit) is a python package that comes with many useful implementations of NLP tools and datasets.\n",
    "\n",
    "From the command line, using pip: `pip3 install nltk` or `pip install nltk`\n",
    "\n",
    "[installing nltk](https://www.nltk.org/install.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# for the tokenizers that we're going to use\n",
    "# won't cause an error if you've already downloaded it\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"N.K. Jemison is a science fiction author. Prof. Felix isn't.\"\n",
    "words = nltk.word_tokenize(example)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_nltk_tokens = nltk.word_tokenize(contents)\n",
    "# feel free to add/edit code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How does the size of the vocabulary for Moby Dick compare when you use `nltk`'s tokenizer vs. the one that you made? __YOUR ANSWER HERE__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Determining Sentiment\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use nltk to access the reviews that we want to classify eventually\n",
    "import nltk\n",
    "import nltk.corpus as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_list(filename):\n",
    "    \"\"\"\n",
    "    Loads a lexicon from a plain text file in the format of one word per line.\n",
    "    Parameters:\n",
    "    filename (str): path to file\n",
    "\n",
    "    Returns:\n",
    "    list: list of words\n",
    "    \"\"\"\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        # skip the header content\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                break\n",
    "        # read the rest of the lines into a list\n",
    "        return [line.strip() for line in f]\n",
    "    # otherwise return an empty list\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the positive and negative word lists here\n",
    "# TODO: the paths to your negative/positive word files here\n",
    "neg_lex = load_word_list(\"NEGATIVE WORD LIST PATH\")\n",
    "pos_lex = load_word_list(\"POSITIVE WORD LIST PATH\")\n",
    "\n",
    "# TODO: How many words are in each list?\n",
    "\n",
    "\n",
    "# TODO: Use python's list slicing to look at the first 10 elements in each list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: which words are in both the positive and the negative lists?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create our rule-based classifier! We have access to the word lists that you loaded and anything else you know about the world (reflect on how you as a human being can tell if a review is positive/negative). Your classifier need not be perfect, but it should be reasonable (don't just say everything is positive!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_classify(tokens, pos_lexicon, neg_lexicon, verbose = False):\n",
    "    \"\"\"\n",
    "    This function classifies a given tokenized text as positive or negative\n",
    "    based on the provided lexicons.\n",
    "    Parameters:\n",
    "    tokens (list) - list of strings tokenized words in the text to classify\n",
    "    pos_lexicon (list) - list of strings words in the positive word lexicon\n",
    "    neg_lexicon (list) - list of strings words in the negative word lexicon\n",
    "    verbose (boolean) - flag indicating whether or not to print verbose (debugging) output. \n",
    "            Default value False.\n",
    "    Returns:\n",
    "    string \"pos\" if the list of tokens is positive overall, \"neg\" if they are negative overall.\n",
    "    \"\"\"\n",
    "    # TODO: implement this function! This is our classifier.  \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we'll test out your classifier!\n",
    "# Here are two example movie reviews.\n",
    "nltk.download('movie_reviews')\n",
    "movies = corpus.movie_reviews\n",
    "\n",
    "# load in a single negative review\n",
    "negative_toks = movies.words('neg/cv001_19502.txt')\n",
    "# uncomment the text below to see the contents of the review\n",
    "# neg_text = \" \".join(negative_toks)\n",
    "# print(neg_text)\n",
    "\n",
    "# load in a single positive review\n",
    "positive_toks = movies.words('pos/cv992_11962.txt')\n",
    "# pos_text = \" \".join(positive_toks)\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# call your rule_based_classify on these example reviews.\n",
    "\n",
    "# Does our classification function label them correctly? Why or why not?\n",
    "# take a look at the contents of the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What labels does your classifier assign these two reviews? __YOUR ANSWER HERE__\n",
    "2. Are these correct? __YOUR ANSWER HERE__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: How good is your sentiment classifier?\n",
    "-----\n",
    "\n",
    "Given the movies dataset from `nltk`, how many of the reviews does your classifier classify correctly?\n",
    "\n",
    "We'll look at three different metrics: __accuracy__, __precision__, and __recall__.\n",
    "\n",
    "__accuracy__: what you think of when you think of correctness.\n",
    "$$ \\frac{\\texttt{number correct}}{\\texttt{total number}}$$\n",
    "\n",
    "Precision and recall require differentiated between the ways in which the classifier can be correct or incorrect. \n",
    "\n",
    "- __true positive__: an example whose gold label is positive and that the classifier labels as positive\n",
    "- __true negative__: an example whose gold label is negative and that the classifier labels as negative\n",
    "- __false positive__: an example whose gold label is negative and that the classifier labels as positive\n",
    "- __false negative__: an example whose gold label is positive and that the classifier labels as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# you can use numpy's random functionality if you'd like to\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the available file ids, this is one way that we can access them.\n",
    "# This will give you a list of neg/positive file ids.\n",
    "print(len(movies.fileids('neg')))\n",
    "# choose 100 random items without replacement from a list\n",
    "print(random.sample(movies.fileids('neg'), 100))\n",
    "print(len(movies.fileids('pos')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Write code that uses your classifier to classify 100 randomly chosen\n",
    "# negative reviews and 100 randomly chosen positive reviews\n",
    "# count the number of true positives, true negatives, false positives, and false negatives\n",
    "\n",
    "# to get the tokens associated with a certain file id,\n",
    "# tokens = movies.words(file_id)\n",
    "\n",
    "# takes a long time to run if you loop over all fileids as opposed to just\n",
    "# 100 randomly chosen ones\n",
    "# make sure you don't classify the same review twice!\n",
    "# (it takes us about 10 seconds to classify 200 reviews on a 2020 macbook air)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# TODO: print out the number of true positives, false positives,\n",
    "# false negatives, and true negatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the equations for accuracy, precision, and recall in terms of what we've just been counting. $tp$ means true positive, $fp$ means false positive, $fn$ means false negative, and $tn$ means true negative.\n",
    "\n",
    "$$ accuracy = \\frac{tp + tn}{tp + fp + fn + tn}$$\n",
    "\n",
    "$$ precision = \\frac{tp}{tp + fp}$$\n",
    "\n",
    "$$ recall = \\frac{tp}{tp + fn}$$\n",
    "\n",
    "You can think of precision as \"how many of my positive guesses were correct?\" and recall as \"how many of the positive examples did I find?\" ðŸ˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate and print accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate and print precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate and print recall\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6: n-gram math\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "def count_list(ls: list) -> dict:\n",
    "    counts = {}\n",
    "    for item in ls:\n",
    "        # we're not going to be clever about counting here,\n",
    "        # no conditionals, no sets, nothing\n",
    "        counts[item] = ls.count(item)\n",
    "    return counts\n",
    "\n",
    "# see the difference between the following two items\n",
    "example = [random.randint(0, 100) for i in range(2000)]\n",
    "start = time.time()\n",
    "count_list(example)\n",
    "end = time.time()\n",
    "print(\"That took:\", end - start, \"seconds!\")\n",
    "\n",
    "# this takes a very similar amount of time to count_dict from HW 1\n",
    "start = time.time()\n",
    "Counter(example)\n",
    "end = time.time()\n",
    "print(\"That took:\", end - start, \"seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put your create_ngrams (or make_ngrams) function here!\n",
    "def make_ngrams(tokens: list, n: int) -> list:\n",
    "    \"\"\"Creates n-grams for the given token sequence.\n",
    "    Args:\n",
    "    tokens (list): a list of tokens as strings\n",
    "    n (int): the length of n-grams to create\n",
    "\n",
    "    Returns:\n",
    "    list: list of tuples of strings, each tuple being one of the individual n-grams\n",
    "    \"\"\"\n",
    "    # TODO: implement this function!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate the bigram score of the following sequence of tokens\n",
    "# for this example, we'll use a \"vanilla\" scoring technique\n",
    "# no Laplace smoothing, no unknown tokens\n",
    "training_data = [\"<s>\", \"I\", \"love\", \"dogs\", \"</s>\", \"<s>\", \"I\", \"love\", \"cats\", \"</s>\", \"<s>\", \"I\", \"love\", \"dinosaurs\", \"</s>\"]\n",
    "\n",
    "# TODO: call your create_ngrams function to get your bigrams\n",
    "\n",
    "\n",
    "\n",
    "to_score = [\"<s>\", \"I\", \"love\", \"cats\", \"</s>\"]\n",
    "start = time.time()\n",
    "\n",
    "# BEGIN SCORING SECTION\n",
    "# start probability at one so that we can multiply the probability of\n",
    "# each subsequent next token with it\n",
    "total_prob = 1\n",
    "for i in range(1, len(to_score)):\n",
    "    # TODO: YOUR SCORE CALCULATION CODE HERE\n",
    "    \n",
    "\n",
    "# END SCORING SECTION\n",
    "end = time.time()\n",
    "\n",
    "# print your final probability\n",
    "print(\"Final probability:\", total_prob)\n",
    "print(\"That took\", end - start, \"seconds!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, pretend that we had a lot more data\n",
    "training_data = [\"<s>\", \"I\", \"love\", \"dogs\", \"</s>\", \"<s>\", \"I\", \"love\", \"cats\", \"</s>\", \"<s>\", \"I\", \"love\", \"dinosaurs\", \"</s>\"]\n",
    "# this is the amount of training data in the berp set(https://www1.icsi.berkeley.edu/Speech/berp.html)\n",
    "training_data = training_data * 3778\n",
    "\n",
    "# TODO: call your create_ngrams function here\n",
    "\n",
    "\n",
    "print(\"Number of training tokens:\", len(training_data))\n",
    "start = time.time()\n",
    "# and what if we had 5000 sentences to score?\n",
    "for example_num in range(3000):\n",
    "    # TODO: COPY AND PASTE YOUR SCORING CODE HERE (between \"BEGIN SCORING SECTION\" and \"END SCORING SECTION\")\n",
    "    # (remove any print statements that you have)\n",
    "    # (make sure it is appropriately indented)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print(\"That took\", end - start, \"seconds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the moral of the story? If you perform your counts at the same time you score, you'll be doing the same work over and over again which will result in a significantly slower model!\n",
    "\n",
    "Make sure that you're gathering the counts that you need in `train` and only performing scoring calculations (as opposed to also counting things) in `score`.\n",
    "\n",
    "This is particularly important when using larger data sets! (berp is not that big)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 7: Calculating tf-idf\n",
    "----\n",
    "\n",
    "To calculate tf-idf, we'll need to first construct a term-document matrix from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(term: str, document: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculate term frequency\n",
    "    Parameters:\n",
    "    term - string\n",
    "    document - list of strings (tokenized document)\n",
    "    Return:\n",
    "    float term frequency\n",
    "    \"\"\"\n",
    "    return np.log10(document.count(term) + 1)\n",
    "\n",
    "def idf(N: int, df_t: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate inverse document frequency given the \n",
    "    number of documents and the number of documents the term appears in.\n",
    "    Paramenters:\n",
    "    N - int (number of documents)\n",
    "    df_t - int (number of documents the term appears in)\n",
    "    Return:\n",
    "    float inverse document frequency\n",
    "    \"\"\"\n",
    "    return np.log10(N / df_t)\n",
    "\n",
    "\n",
    "def term_in_documents(term: str, documents: list) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the number of documents in a list of documents that a target\n",
    "    term appears in.\n",
    "    Parameters:\n",
    "    term - str\n",
    "    documents - list of list of str (list of tokenized documents)\n",
    "    Return:\n",
    "    int number of documents the term appears in\n",
    "    \"\"\"\n",
    "    #TODO: IMPLEMENT ME\n",
    "    return 0\n",
    "\n",
    "\n",
    "# load in the data\n",
    "def load_tokens(filename):\n",
    "    f = open(filename, 'r', encoding=\"utf-8\")\n",
    "    contents = f.read().lower()\n",
    "    f.close()\n",
    "    # if you don't have nltk installed, use another tokenization\n",
    "    # strategy here like str.split()\n",
    "    return nltk.word_tokenize(contents)\n",
    "\n",
    "mobydick = load_tokens('path to mobydick') \n",
    "# shakes = load_tokens('path to shakespeare')\n",
    "# pandp = load_tokens('path to pride and prejudice')\n",
    "# books = [mobydick, shakes, pandp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we won't create the entire term-document matrix, we'll just do it for a few key terms that we\n",
    "# care about for the sake of time\n",
    "# TODO: pick 3 - 5 words\n",
    "words = ['your words here']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the term-document matrix\n",
    "matrix = []\n",
    "\n",
    "# iterate through your chosen words\n",
    "for word in words:\n",
    "    # count how many times it occurs in each book\n",
    "    # this is just for debugging for you\n",
    "    counts = [str(b.count(word)) for b in books]\n",
    "    print(word, \":\\t\", \"\\t\".join(counts))\n",
    "    \n",
    "    \n",
    "    # calculate the term frequency for each book\n",
    "    # for this word\n",
    "    # tf_words should be the same length as counts\n",
    "#     tf_words = YOUR CODE HERE\n",
    "    \n",
    "    # calculate idf for this term\n",
    "    # this will be a single scalar\n",
    "    N = len(books)\n",
    "    df_t = term_in_documents(word, books)\n",
    "#     idf_t = YOUR CODE HERE\n",
    "    \n",
    "    # multiply tf with idf for each book/each\n",
    "    # term frequency you calculated\n",
    "#     tfidf_words = YOUR CODE HERE\n",
    "    \n",
    "    # add the tfidf numbers to your matrix\n",
    "    matrix.append(tfidf_words)\n",
    "    \n",
    "    # uncomment to see visually the different components (helpful for debugging)\n",
    "#     print(word, \" tf:\\t\", \"\\t\".join([str(x) for x in tf_words]))\n",
    "#     print(word, \"idf:\\t\", idf_t)\n",
    "#     print(word, \" tf-idf:\\t\", \"\\t\".join([str(x) for x in tfidf_words]))\n",
    "    \n",
    "# if you'd like to, uncomment the following code to make the matrix into a numpy array\n",
    "# matrix = np.array(matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What should the dimensions of your matrix be? __YOUR ANSWER HERE__\n",
    "2. What happens if you attempt to calculate tfidf of a term that exists in *none* of your books? __YOUR ANSWER HERE__\n",
    "2. What happens if you attempt to calculate tfidf of a term that exists in *all* of your books? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dimensions of your matrix\n",
    "# number of rows should match number of words\n",
    "# number of cols should match number of documents (books)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(v1: list, v2: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two vectors of the same length.\n",
    "    Parameters:\n",
    "    v1 - list (of numbers)\n",
    "    v2 - list (of numbers)\n",
    "    Return:\n",
    "    float cosine similarity\n",
    "    \"\"\"\n",
    "    # you may find the numpy functions np.dot() and np.linalg.norm() useful\n",
    "    # TODO: implement\n",
    "    return 0\n",
    "\n",
    "# calculate the similarity between two *word* vectors\n",
    "# we'll just do word vectors because unless matrix is a numpy array\n",
    "# it is (more) difficult to get column vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you finish, which book is closest to moby dick?\n",
    "# you'll need a *column* vector here (instead of a row vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you finish finish, which book is closest to moby dick, but remake your matrix with all vocabulary terms?\n",
    "# create the term-document matrix\n",
    "# you'll want to use counters for each book's vocabulary for the sake of efficiency\n",
    "\n",
    "\n",
    "# you may want to re-define new counter versions of your tf, idf, term_in_documents \n",
    "# functions so that they work with counters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the shape of your matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which book is actually closest to moby dick?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13 (main, Nov  1 2023, 16:33:59) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
