{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzLe41IhaQsG"
      },
      "source": [
        "Lab 2: Intro to Neural Nets\n",
        "===============\n",
        "\n",
        "2/14/24, CS 6120 Natural Language Processing\\\n",
        "Complete in groups of up to 3.\\\n",
        "Please indicate your partners on the Gradescope submission. Due Fri 2/16, 11:59 PM.\n",
        "\n",
        "-------\n",
        "version 1: Fall 2023, Felix Muzny\\\n",
        "version 2: Spring 2024, Raj Venkat - modified training of NN from scratch to include MSE, gradients computed using computational graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qb2ektNaQsH"
      },
      "source": [
        "Task 0: Group Member Names:\n",
        "-----------------\n",
        "1. Bhuvan Karthik Channagiri  \n",
        "2. Nikita Mandal    \n",
        "3. Vaibhav Kejriwal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RJa85smaQsI"
      },
      "source": [
        "Task 1: Writing a neural net from scratch\n",
        "-----------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OPF6QCCkaQsI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "# seed random number generation so that you can\n",
        "# track the same numbers as each other\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z3A9DRAUaQsJ"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x: float) -> float:\n",
        "    \"\"\"\n",
        "    Apply the sigmoid function to the passed-in value.\n",
        "    Parameters:\n",
        "        x - float value to pass through sigmoid\n",
        "    Return:\n",
        "    float in [0, 1]\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_deriv(x: float) -> float:\n",
        "    \"\"\"\n",
        "    Apply the derivative of the sigmoid function\n",
        "    sigmoid(x) * (1 - sigmoid(x))\n",
        "    to the passed-in value.\n",
        "    Parameters:\n",
        "        x - float value to pass through sigmoid derivative\n",
        "    Return:\n",
        "    float result\n",
        "    \"\"\"\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x59-BrOFaQsJ"
      },
      "outputs": [],
      "source": [
        "# input dataset\n",
        "# 3rd \"feature\" is the bias term\n",
        "X = np.array([  [0,0,1],\n",
        "                [0,1,1],\n",
        "                [1,0,1],\n",
        "                [1,1,1] ])\n",
        "\n",
        "# labels, transposed so that they match\n",
        "# easily with our inputs X\n",
        "# the first label matches the first row in our input data,\n",
        "# the second label matches the second row in our input data, etc\n",
        "# .T gets the transpose for us, which makes\n",
        "# matrix math easier later\n",
        "y = np.array([[0,1,1,0]]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvvCQQfAaQsJ"
      },
      "source": [
        "1. What logical function (AND, OR, etc) does this dataset represent? (remember that this function should apply to two inputs (our two input features and produce the matching label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-14SYBs3aQsJ"
      },
      "source": [
        "Ans: It represents the XOR logical function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irwe6B_0aQsK",
        "outputId": "7232be3f-10e8-42e2-d2e1-3a983de137cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W: [[-0.25091976  0.90142861  0.46398788  0.19731697]\n",
            " [-0.68796272 -0.68801096 -0.88383278  0.73235229]\n",
            " [ 0.20223002  0.41614516 -0.95883101  0.9398197 ]]\n",
            "U: [[ 0.66488528]\n",
            " [-0.57532178]\n",
            " [-0.63635007]\n",
            " [-0.63319098]]\n",
            "Epoch 0, Loss: 0.2817889399227034\n",
            "Epoch 10, Loss: 0.2836173417103767\n",
            "Epoch 20, Loss: 0.2855429618593159\n",
            "Epoch 30, Loss: 0.28756938974584956\n",
            "Epoch 40, Loss: 0.2897001389991346\n",
            "Epoch 50, Loss: 0.29193861439643004\n",
            "Epoch 60, Loss: 0.29428807532092227\n",
            "Epoch 70, Loss: 0.2967515957669625\n",
            "Epoch 80, Loss: 0.29933202094390293\n",
            "Epoch 90, Loss: 0.3020319206087082\n",
            "Updated W: [[-0.26700583  0.91619158  0.48154705  0.2149686 ]\n",
            " [-0.70244192 -0.67275331 -0.86776271  0.74695849]\n",
            " [ 0.17282266  0.444371   -0.92694725  0.9697393 ]]\n",
            "Updated U: [[ 0.58131536]\n",
            " [-0.69687905]\n",
            " [-0.69209191]\n",
            " [-0.79272875]]\n"
          ]
        }
      ],
      "source": [
        "hidden_units = 4\n",
        "input_features = X.shape[1]\n",
        "\n",
        "# TODO: fill in dimensions here for W\n",
        "# Fill these in as a tuple like (rows, columns)\n",
        "# This corresponds to how shapes are represented for numpy arrays\n",
        "\n",
        "W_dim = (input_features, hidden_units)\n",
        "\n",
        "# Initialize weights W randomly with mean 0 and range [-1, 1]\n",
        "# Use W_dim to produce the correct number of random numbers\n",
        "\n",
        "W = np.random.uniform(-1, 1, W_dim)\n",
        "\n",
        "# Similarly, fill in dimensions for U\n",
        "# Note that since we are doing binary classification, the second dimension here should be 1\n",
        "# (corresponding to one output unit)\n",
        "\n",
        "U_dim = (hidden_units, 1)\n",
        "\n",
        "# Initialize weights U randomly with mean 0 and range [-1, 1]\n",
        "# Use U_dim to produce the correct number of random numbers\n",
        "\n",
        "U = np.random.uniform(-1, 1, U_dim)\n",
        "\n",
        "print(\"W:\", W)\n",
        "print(\"U:\", U)\n",
        "\n",
        "# TRAINING LOOP\n",
        "\n",
        "inputs = X\n",
        "learning_rate=0.01\n",
        "num_epochs = 100  # Play around with this\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    # forward propagation â€” using sigmoid\n",
        "\n",
        "    h = sigmoid(np.dot(inputs,W))\n",
        "\n",
        "    # note that this gives us the classification for every input\n",
        "    # example simultaneously\n",
        "\n",
        "    y_hat = sigmoid(np.dot(h,U))\n",
        "\n",
        "    error = (y - y_hat)\n",
        "\n",
        "    loss = np.mean(error ** 2) # FILL IN THE CODE TO COMPUTE THE MEAN SQUARED ERROR LOSS\n",
        "\n",
        "    # Fill in computations for the gradients below. Deduce the formula for the gradients\n",
        "    # by drawing a computation graph (you don't need to submit the graph)\n",
        "\n",
        "    grad_loss_mse_U = np.dot(h.T, error * sigmoid_deriv(y_hat))# FILL ME IN\n",
        "    grad_loss_mse_W = np.dot(inputs.T, np.dot(error * sigmoid_deriv(y_hat), U.T) * sigmoid_deriv(h))# FILL ME IN\n",
        "\n",
        "    # In the lines below, you may need to transpose some matrices\n",
        "\n",
        "    W = W - learning_rate * grad_loss_mse_W\n",
        "    U = U - learning_rate * grad_loss_mse_U\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(f'Epoch {i}, Loss: {loss}')\n",
        "\n",
        "print(\"Updated W:\", W)\n",
        "print(\"Updated U:\", U)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruWxnaz8aQsK"
      },
      "source": [
        "2. Does the hidden layer have a bias term in this neural net? __NO, here the hidden layer does not have a bias term explicitly as in this Neural network the bias term is in the input layer  only.__\n",
        "3. What variables' values are updated as the loop above iterates? __The variables that are updated in the loop are the two sets of weights, one which is between the input layer and hidden layer, represented by **(W)** and the other one between the hidden layer and the output layer,represented by **(U)**. In the learning process we aim to minimize the loss by adjusting these weights. By minimizing the loss the model performance can be improved. This can also be called Backward Propogation in other terms__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyhfYok3aQsL",
        "outputId": "ec452863-d73d-42bc-c622-936ef5099786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output After Training:\n",
            "Epoch 0, Loss: 0.2661199320226466\n",
            "Epoch 100, Loss: 0.24975095281828957\n",
            "Epoch 200, Loss: 0.24960796645186184\n",
            "Epoch 300, Loss: 0.24946288721328233\n",
            "Epoch 400, Loss: 0.24930086926250977\n",
            "Epoch 500, Loss: 0.24910994772142533\n",
            "Epoch 600, Loss: 0.24887743580052007\n",
            "Epoch 700, Loss: 0.2485874795256049\n",
            "Epoch 800, Loss: 0.24821859719365916\n",
            "Epoch 900, Loss: 0.2477404100196957\n",
            "Epoch 1000, Loss: 0.24710869284402948\n",
            "Epoch 1100, Loss: 0.24625746210462884\n",
            "Epoch 1200, Loss: 0.24508601217606268\n",
            "Epoch 1300, Loss: 0.24343748287501565\n",
            "Epoch 1400, Loss: 0.2410637020376533\n",
            "Epoch 1500, Loss: 0.23756957364699488\n",
            "Epoch 1600, Loss: 0.23233309827324544\n",
            "Epoch 1700, Loss: 0.22441603016775857\n",
            "Epoch 1800, Loss: 0.21253937770402806\n",
            "Epoch 1900, Loss: 0.1953153478198205\n",
            "Predicted probabilities: [[0.38844237]\n",
            " [0.58102369]\n",
            " [0.56983453]\n",
            " [0.42021627]]\n",
            "Predicted labels: [0, 1, 1, 0]\n",
            "\n",
            "Predicted probabilities: [[0.38844237]\n",
            " [0.58102369]\n",
            " [0.56983453]\n",
            " [0.42021627]]\n",
            "Predicted labels: [0, 1, 1, 0]\n",
            "Actual labels: [[0 1 1 0]]\n",
            "Actual labels: [[0 1 1 0]]\n",
            "Assigned probabilities: [[0.388777  ]\n",
            " [0.5807387 ]\n",
            " [0.56954301]\n",
            " [0.42056815]]\n",
            "Assigned labels: [0, 1, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "print(\"Output After Training:\")\n",
        "\n",
        "# These are the same as the inputs that we trained this net on\n",
        "test_inputs = np.array([  [0,0,1],\n",
        "                [0,1,1],\n",
        "                [1,0,1],\n",
        "                [1,1,1] ])\n",
        "gold_labels = np.array([[0,1,1,0]]).T\n",
        "\n",
        "# Initialize weights randomly\n",
        "np.random.seed(0)\n",
        "W = 2 * np.random.random((3, 4)) - 1\n",
        "U = 2 * np.random.random((4, 1)) - 1\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.5\n",
        "num_epochs = 2000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward propagation\n",
        "    h = sigmoid(np.dot(X, W))\n",
        "    y_hat = sigmoid(np.dot(h, U))\n",
        "\n",
        "    # Backpropagation\n",
        "    error = y - y_hat\n",
        "    dU = np.dot(h.T, error * sigmoid_deriv(y_hat))\n",
        "    dW = np.dot(X.T, np.dot(error * sigmoid_deriv(y_hat), U.T) * sigmoid_deriv(h))\n",
        "\n",
        "    # Update weights\n",
        "    U += learning_rate * dU\n",
        "    W += learning_rate * dW\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = np.mean((y - y_hat) ** 2)\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Test the trained model\n",
        "test_inputs = np.array([[0, 0, 1],\n",
        "                        [0, 1, 1],\n",
        "                        [1, 0, 1],\n",
        "                        [1, 1, 1]])\n",
        "\n",
        "h_test = sigmoid(np.dot(test_inputs, W))\n",
        "y_pred = sigmoid(np.dot(h_test, U))\n",
        "\n",
        "print(\"Predicted probabilities:\", y_pred)\n",
        "print(\"Predicted labels:\", [1 if val > 0.5 else 0 for val in y_pred])\n",
        "\n",
        "# Test the model after training\n",
        "h_test = sigmoid(np.dot(test_inputs, W))\n",
        "y_pred = sigmoid(np.dot(h_test, U))\n",
        "\n",
        "print(\"\\nPredicted probabilities:\", y_pred)\n",
        "print(\"Predicted labels:\", [1 if val > 0.5 else 0 for val in y_pred])\n",
        "print(\"Actual labels:\", y.T)\n",
        "print(\"Actual labels:\", gold_labels.T)\n",
        "print(\"Assigned probabilities:\", y_hat)\n",
        "print(\"Assigned labels:\", [1 if y_hat_val > .5 else 0 for y_hat_val in y_hat])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bazst5bTaQsL"
      },
      "source": [
        "4. How many iterations did you need for the predicted values $\\hat y$ to match the actual values? __The number of iterations needed was 2000__\n",
        "5. Make a graph of how the `layer2_error` changes as epochs progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "q0Dptc2ZaQsL",
        "outputId": "1208987e-f44b-4b19-8dd6-2e93bac4467e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "hidden_layer_errors = []\n",
        "\n",
        "#using same num of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward propagation\n",
        "    h = sigmoid(np.dot(inputs, W))\n",
        "    y_hat = sigmoid(np.dot(h, U))\n",
        "\n",
        "    error = y - y_hat\n",
        "\n",
        "    # Mean Squared Error Loss\n",
        "    loss = np.mean(error**2)\n",
        "\n",
        "    # Append the error to the list\n",
        "    hidden_layer_errors.append(np.mean((h - y_hat)**2))\n",
        "\n",
        "    # Gradients\n",
        "    grad_loss_mse_U = np.dot(h.T, error * sigmoid_deriv(y_hat))\n",
        "    grad_loss_mse_W = np.dot(inputs.T, np.dot(error * sigmoid_deriv(y_hat), U.T) * sigmoid_deriv(h))\n",
        "\n",
        "    # Update weights\n",
        "    U = U - learning_rate * grad_loss_mse_U\n",
        "    W = W - learning_rate * grad_loss_mse_W\n",
        "\n",
        "# Plotting the error over epochs\n",
        "plt.plot(range(num_epochs), hidden_layer_errors)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE in hidden layer: Layer 2)')\n",
        "plt.title('Change in MSE over Epochs for Layer 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqfAttlzaQsL"
      },
      "source": [
        "Task 2: Neural Nets from libraries (you'll be doing a similar thing in your sentiment analysis HW!)\n",
        "----------------\n",
        "\n",
        "Now, we'll take a look at some common libraries used to create classifiers using neural nets. We'll take a look at [`keras`](https://keras.io/) which provides a nice API for implementing neural nets and can be run on top of TensorFlow, CNTK, or Theano. We'll look at an example using [`tensorflow`](https://github.com/tensorflow/tensorflow) as our backend.\n",
        "\n",
        "Installation of component libraries:\n",
        "\n",
        "```\n",
        "pip3 install tensorflow\n",
        "sudo pip3 install keras\n",
        "```\n",
        "\n",
        "If you are working on a Silicon chip Mac (Macs with M1 and M2 chips), you'll need at least OS 12.0+ (Monterey (12) or Ventura (13)), then you'll want to follow the [instructions on the Apple developers website](https://developer.apple.com/metal/tensorflow-plugin/). We will be using tensorflow/keras going forward, so this is worth doing on your own outside of class!\n",
        "\n",
        "In the meantime, you can also upload this notebook to [Google colaboratory](https://colab.research.google.com/) and run this portion on the cloud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "X8XisOeIaQsL"
      },
      "outputs": [],
      "source": [
        "# Uncomment these lines of code to do the import. Left\n",
        "# commented because on Macs with unsupported architecture, these\n",
        "# imports will kill your kernel which is highly annoying.\n",
        "\n",
        "\n",
        "# Sequential will be the base model we'll use\n",
        "from keras.models import Sequential\n",
        "# Dense layers are our base feed forward layers\n",
        "from keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxyZUGEwaQsM",
        "outputId": "d9a78ce9-13ae-4c25-9281-6b670eab944d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/450\n",
            "1/1 [==============================] - 0s 386ms/step - loss: 0.7236 - accuracy: 0.2500\n",
            "Epoch 2/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7229 - accuracy: 0.2500\n",
            "Epoch 3/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7222 - accuracy: 0.2500\n",
            "Epoch 4/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7214 - accuracy: 0.2500\n",
            "Epoch 5/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7207 - accuracy: 0.2500\n",
            "Epoch 6/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7200 - accuracy: 0.2500\n",
            "Epoch 7/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7192 - accuracy: 0.2500\n",
            "Epoch 8/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7185 - accuracy: 0.2500\n",
            "Epoch 9/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7178 - accuracy: 0.2500\n",
            "Epoch 10/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7171 - accuracy: 0.2500\n",
            "Epoch 11/450\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7164 - accuracy: 0.2500\n",
            "Epoch 12/450\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7157 - accuracy: 0.2500\n",
            "Epoch 13/450\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7150 - accuracy: 0.2500\n",
            "Epoch 14/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7143 - accuracy: 0.2500\n",
            "Epoch 15/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7137 - accuracy: 0.2500\n",
            "Epoch 16/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7129 - accuracy: 0.2500\n",
            "Epoch 17/450\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7115 - accuracy: 0.2500\n",
            "Epoch 18/450\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7101 - accuracy: 0.2500\n",
            "Epoch 19/450\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7087 - accuracy: 0.2500\n",
            "Epoch 20/450\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7074 - accuracy: 0.2500\n",
            "Epoch 21/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7061 - accuracy: 0.2500\n",
            "Epoch 22/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7047 - accuracy: 0.2500\n",
            "Epoch 23/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7034 - accuracy: 0.2500\n",
            "Epoch 24/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7021 - accuracy: 0.2500\n",
            "Epoch 25/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7009 - accuracy: 0.2500\n",
            "Epoch 26/450\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6996 - accuracy: 0.5000\n",
            "Epoch 27/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6983 - accuracy: 0.5000\n",
            "Epoch 28/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6971 - accuracy: 0.5000\n",
            "Epoch 29/450\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6958 - accuracy: 0.5000\n",
            "Epoch 30/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6946 - accuracy: 0.5000\n",
            "Epoch 31/450\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6934 - accuracy: 0.5000\n",
            "Epoch 32/450\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6922 - accuracy: 0.5000\n",
            "Epoch 33/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6910 - accuracy: 0.5000\n",
            "Epoch 34/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6898 - accuracy: 0.5000\n",
            "Epoch 35/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6887 - accuracy: 0.5000\n",
            "Epoch 36/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6875 - accuracy: 0.5000\n",
            "Epoch 37/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6863 - accuracy: 0.5000\n",
            "Epoch 38/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6852 - accuracy: 0.5000\n",
            "Epoch 39/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6841 - accuracy: 0.5000\n",
            "Epoch 40/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6830 - accuracy: 0.5000\n",
            "Epoch 41/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6818 - accuracy: 0.5000\n",
            "Epoch 42/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6807 - accuracy: 0.5000\n",
            "Epoch 43/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6796 - accuracy: 0.5000\n",
            "Epoch 44/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6786 - accuracy: 0.5000\n",
            "Epoch 45/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6775 - accuracy: 0.5000\n",
            "Epoch 46/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6764 - accuracy: 0.5000\n",
            "Epoch 47/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6754 - accuracy: 0.5000\n",
            "Epoch 48/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6743 - accuracy: 0.5000\n",
            "Epoch 49/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6733 - accuracy: 0.5000\n",
            "Epoch 50/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6722 - accuracy: 0.5000\n",
            "Epoch 51/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6712 - accuracy: 0.5000\n",
            "Epoch 52/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6702 - accuracy: 0.5000\n",
            "Epoch 53/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6692 - accuracy: 0.5000\n",
            "Epoch 54/450\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6682 - accuracy: 0.5000\n",
            "Epoch 55/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6672 - accuracy: 0.5000\n",
            "Epoch 56/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6662 - accuracy: 0.5000\n",
            "Epoch 57/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6652 - accuracy: 0.5000\n",
            "Epoch 58/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6643 - accuracy: 0.5000\n",
            "Epoch 59/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6633 - accuracy: 0.5000\n",
            "Epoch 60/450\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6623 - accuracy: 0.5000\n",
            "Epoch 61/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6614 - accuracy: 0.5000\n",
            "Epoch 62/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6605 - accuracy: 0.5000\n",
            "Epoch 63/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6595 - accuracy: 0.5000\n",
            "Epoch 64/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6586 - accuracy: 0.5000\n",
            "Epoch 65/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6577 - accuracy: 0.5000\n",
            "Epoch 66/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6567 - accuracy: 0.5000\n",
            "Epoch 67/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6558 - accuracy: 0.5000\n",
            "Epoch 68/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6549 - accuracy: 0.5000\n",
            "Epoch 69/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6540 - accuracy: 0.5000\n",
            "Epoch 70/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6531 - accuracy: 0.5000\n",
            "Epoch 71/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6523 - accuracy: 0.5000\n",
            "Epoch 72/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6514 - accuracy: 0.5000\n",
            "Epoch 73/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6505 - accuracy: 0.5000\n",
            "Epoch 74/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6496 - accuracy: 0.5000\n",
            "Epoch 75/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6488 - accuracy: 0.5000\n",
            "Epoch 76/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6479 - accuracy: 0.5000\n",
            "Epoch 77/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6471 - accuracy: 0.5000\n",
            "Epoch 78/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6462 - accuracy: 0.5000\n",
            "Epoch 79/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6454 - accuracy: 0.5000\n",
            "Epoch 80/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6445 - accuracy: 0.5000\n",
            "Epoch 81/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6437 - accuracy: 0.5000\n",
            "Epoch 82/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6429 - accuracy: 0.5000\n",
            "Epoch 83/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6421 - accuracy: 0.7500\n",
            "Epoch 84/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6412 - accuracy: 0.7500\n",
            "Epoch 85/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6404 - accuracy: 0.7500\n",
            "Epoch 86/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6396 - accuracy: 0.7500\n",
            "Epoch 87/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6388 - accuracy: 0.7500\n",
            "Epoch 88/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6380 - accuracy: 0.7500\n",
            "Epoch 89/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6372 - accuracy: 0.7500\n",
            "Epoch 90/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6365 - accuracy: 0.7500\n",
            "Epoch 91/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6357 - accuracy: 0.7500\n",
            "Epoch 92/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6349 - accuracy: 0.7500\n",
            "Epoch 93/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6341 - accuracy: 1.0000\n",
            "Epoch 94/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6334 - accuracy: 1.0000\n",
            "Epoch 95/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6326 - accuracy: 1.0000\n",
            "Epoch 96/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6318 - accuracy: 1.0000\n",
            "Epoch 97/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6311 - accuracy: 1.0000\n",
            "Epoch 98/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6307 - accuracy: 1.0000\n",
            "Epoch 99/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6302 - accuracy: 1.0000\n",
            "Epoch 100/450\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6297 - accuracy: 1.0000\n",
            "Epoch 101/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6292 - accuracy: 1.0000\n",
            "Epoch 102/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6288 - accuracy: 1.0000\n",
            "Epoch 103/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6284 - accuracy: 1.0000\n",
            "Epoch 104/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6279 - accuracy: 1.0000\n",
            "Epoch 105/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6274 - accuracy: 1.0000\n",
            "Epoch 106/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6269 - accuracy: 1.0000\n",
            "Epoch 107/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6265 - accuracy: 1.0000\n",
            "Epoch 108/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6260 - accuracy: 1.0000\n",
            "Epoch 109/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6256 - accuracy: 1.0000\n",
            "Epoch 110/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6251 - accuracy: 1.0000\n",
            "Epoch 111/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6246 - accuracy: 1.0000\n",
            "Epoch 112/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6242 - accuracy: 1.0000\n",
            "Epoch 113/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6237 - accuracy: 1.0000\n",
            "Epoch 114/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6233 - accuracy: 1.0000\n",
            "Epoch 115/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6228 - accuracy: 1.0000\n",
            "Epoch 116/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6223 - accuracy: 1.0000\n",
            "Epoch 117/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6219 - accuracy: 1.0000\n",
            "Epoch 118/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6214 - accuracy: 1.0000\n",
            "Epoch 119/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6210 - accuracy: 1.0000\n",
            "Epoch 120/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6205 - accuracy: 1.0000\n",
            "Epoch 121/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6201 - accuracy: 1.0000\n",
            "Epoch 122/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6196 - accuracy: 1.0000\n",
            "Epoch 123/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6192 - accuracy: 1.0000\n",
            "Epoch 124/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6187 - accuracy: 1.0000\n",
            "Epoch 125/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6184 - accuracy: 1.0000\n",
            "Epoch 126/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6181 - accuracy: 1.0000\n",
            "Epoch 127/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6178 - accuracy: 1.0000\n",
            "Epoch 128/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6174 - accuracy: 1.0000\n",
            "Epoch 129/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6171 - accuracy: 1.0000\n",
            "Epoch 130/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6169 - accuracy: 1.0000\n",
            "Epoch 131/450\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6167 - accuracy: 1.0000\n",
            "Epoch 132/450\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6163 - accuracy: 1.0000\n",
            "Epoch 133/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6160 - accuracy: 1.0000\n",
            "Epoch 134/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6160 - accuracy: 1.0000\n",
            "Epoch 135/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6155 - accuracy: 1.0000\n",
            "Epoch 136/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6152 - accuracy: 1.0000\n",
            "Epoch 137/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6150 - accuracy: 1.0000\n",
            "Epoch 138/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6147 - accuracy: 1.0000\n",
            "Epoch 139/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6147 - accuracy: 1.0000\n",
            "Epoch 140/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6143 - accuracy: 1.0000\n",
            "Epoch 141/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6141 - accuracy: 1.0000\n",
            "Epoch 142/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6138 - accuracy: 1.0000\n",
            "Epoch 143/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6135 - accuracy: 1.0000\n",
            "Epoch 144/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6133 - accuracy: 1.0000\n",
            "Epoch 145/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6129 - accuracy: 1.0000\n",
            "Epoch 146/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6129 - accuracy: 1.0000\n",
            "Epoch 147/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6124 - accuracy: 1.0000\n",
            "Epoch 148/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6124 - accuracy: 1.0000\n",
            "Epoch 149/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6118 - accuracy: 1.0000\n",
            "Epoch 150/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6119 - accuracy: 1.0000\n",
            "Epoch 151/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6114 - accuracy: 1.0000\n",
            "Epoch 152/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6113 - accuracy: 1.0000\n",
            "Epoch 153/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6109 - accuracy: 1.0000\n",
            "Epoch 154/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6107 - accuracy: 1.0000\n",
            "Epoch 155/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6104 - accuracy: 1.0000\n",
            "Epoch 156/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6101 - accuracy: 1.0000\n",
            "Epoch 157/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6100 - accuracy: 1.0000\n",
            "Epoch 158/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6095 - accuracy: 1.0000\n",
            "Epoch 159/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6095 - accuracy: 1.0000\n",
            "Epoch 160/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6090 - accuracy: 1.0000\n",
            "Epoch 161/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6087 - accuracy: 1.0000\n",
            "Epoch 162/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6085 - accuracy: 1.0000\n",
            "Epoch 163/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6085 - accuracy: 1.0000\n",
            "Epoch 164/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6080 - accuracy: 1.0000\n",
            "Epoch 165/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6079 - accuracy: 1.0000\n",
            "Epoch 166/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6075 - accuracy: 1.0000\n",
            "Epoch 167/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6073 - accuracy: 1.0000\n",
            "Epoch 168/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6071 - accuracy: 1.0000\n",
            "Epoch 169/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6067 - accuracy: 1.0000\n",
            "Epoch 170/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6066 - accuracy: 1.0000\n",
            "Epoch 171/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6061 - accuracy: 1.0000\n",
            "Epoch 172/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6061 - accuracy: 1.0000\n",
            "Epoch 173/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6056 - accuracy: 1.0000\n",
            "Epoch 174/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6053 - accuracy: 1.0000\n",
            "Epoch 175/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6051 - accuracy: 1.0000\n",
            "Epoch 176/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6051 - accuracy: 1.0000\n",
            "Epoch 177/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6046 - accuracy: 1.0000\n",
            "Epoch 178/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6045 - accuracy: 1.0000\n",
            "Epoch 179/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6041 - accuracy: 1.0000\n",
            "Epoch 180/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6039 - accuracy: 1.0000\n",
            "Epoch 181/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6037 - accuracy: 1.0000\n",
            "Epoch 182/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6033 - accuracy: 1.0000\n",
            "Epoch 183/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6032 - accuracy: 1.0000\n",
            "Epoch 184/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6027 - accuracy: 1.0000\n",
            "Epoch 185/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6027 - accuracy: 1.0000\n",
            "Epoch 186/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6022 - accuracy: 1.0000\n",
            "Epoch 187/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6019 - accuracy: 1.0000\n",
            "Epoch 188/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6017 - accuracy: 1.0000\n",
            "Epoch 189/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6016 - accuracy: 1.0000\n",
            "Epoch 190/450\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6012 - accuracy: 1.0000\n",
            "Epoch 191/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6011 - accuracy: 1.0000\n",
            "Epoch 192/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6007 - accuracy: 1.0000\n",
            "Epoch 193/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6005 - accuracy: 1.0000\n",
            "Epoch 194/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6003 - accuracy: 1.0000\n",
            "Epoch 195/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5999 - accuracy: 1.0000\n",
            "Epoch 196/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5998 - accuracy: 1.0000\n",
            "Epoch 197/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5993 - accuracy: 1.0000\n",
            "Epoch 198/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5993 - accuracy: 1.0000\n",
            "Epoch 199/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5987 - accuracy: 1.0000\n",
            "Epoch 200/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5988 - accuracy: 1.0000\n",
            "Epoch 201/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5982 - accuracy: 1.0000\n",
            "Epoch 202/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5982 - accuracy: 1.0000\n",
            "Epoch 203/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5978 - accuracy: 1.0000\n",
            "Epoch 204/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5976 - accuracy: 1.0000\n",
            "Epoch 205/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5973 - accuracy: 1.0000\n",
            "Epoch 206/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5970 - accuracy: 1.0000\n",
            "Epoch 207/450\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5968 - accuracy: 1.0000\n",
            "Epoch 208/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5964 - accuracy: 1.0000\n",
            "Epoch 209/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5964 - accuracy: 1.0000\n",
            "Epoch 210/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5958 - accuracy: 1.0000\n",
            "Epoch 211/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5956 - accuracy: 1.0000\n",
            "Epoch 212/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5953 - accuracy: 1.0000\n",
            "Epoch 213/450\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5953 - accuracy: 1.0000\n",
            "Epoch 214/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5948 - accuracy: 1.0000\n",
            "Epoch 215/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5947 - accuracy: 1.0000\n",
            "Epoch 216/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5943 - accuracy: 1.0000\n",
            "Epoch 217/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5941 - accuracy: 1.0000\n",
            "Epoch 218/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5939 - accuracy: 1.0000\n",
            "Epoch 219/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5935 - accuracy: 1.0000\n",
            "Epoch 220/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5934 - accuracy: 1.0000\n",
            "Epoch 221/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5929 - accuracy: 1.0000\n",
            "Epoch 222/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5929 - accuracy: 1.0000\n",
            "Epoch 223/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5923 - accuracy: 1.0000\n",
            "Epoch 224/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5921 - accuracy: 1.0000\n",
            "Epoch 225/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5918 - accuracy: 1.0000\n",
            "Epoch 226/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5918 - accuracy: 1.0000\n",
            "Epoch 227/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5913 - accuracy: 1.0000\n",
            "Epoch 228/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5912 - accuracy: 1.0000\n",
            "Epoch 229/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5909 - accuracy: 1.0000\n",
            "Epoch 230/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5906 - accuracy: 1.0000\n",
            "Epoch 231/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5904 - accuracy: 1.0000\n",
            "Epoch 232/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5900 - accuracy: 1.0000\n",
            "Epoch 233/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5899 - accuracy: 1.0000\n",
            "Epoch 234/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5894 - accuracy: 1.0000\n",
            "Epoch 235/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5894 - accuracy: 1.0000\n",
            "Epoch 236/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5888 - accuracy: 1.0000\n",
            "Epoch 237/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5886 - accuracy: 1.0000\n",
            "Epoch 238/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5883 - accuracy: 1.0000\n",
            "Epoch 239/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5883 - accuracy: 1.0000\n",
            "Epoch 240/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5878 - accuracy: 1.0000\n",
            "Epoch 241/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5877 - accuracy: 1.0000\n",
            "Epoch 242/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5874 - accuracy: 1.0000\n",
            "Epoch 243/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5871 - accuracy: 1.0000\n",
            "Epoch 244/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5869 - accuracy: 1.0000\n",
            "Epoch 245/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5865 - accuracy: 1.0000\n",
            "Epoch 246/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5864 - accuracy: 1.0000\n",
            "Epoch 247/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5859 - accuracy: 1.0000\n",
            "Epoch 248/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5859 - accuracy: 1.0000\n",
            "Epoch 249/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5853 - accuracy: 1.0000\n",
            "Epoch 250/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5851 - accuracy: 1.0000\n",
            "Epoch 251/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5851 - accuracy: 1.0000\n",
            "Epoch 252/450\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5846 - accuracy: 1.0000\n",
            "Epoch 253/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5845 - accuracy: 1.0000\n",
            "Epoch 254/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5841 - accuracy: 1.0000\n",
            "Epoch 255/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5839 - accuracy: 1.0000\n",
            "Epoch 256/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5836 - accuracy: 1.0000\n",
            "Epoch 257/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5832 - accuracy: 1.0000\n",
            "Epoch 258/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5831 - accuracy: 1.0000\n",
            "Epoch 259/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5826 - accuracy: 1.0000\n",
            "Epoch 260/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5823 - accuracy: 1.0000\n",
            "Epoch 261/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5821 - accuracy: 1.0000\n",
            "Epoch 262/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5818 - accuracy: 1.0000\n",
            "Epoch 263/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5818 - accuracy: 1.0000\n",
            "Epoch 264/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5813 - accuracy: 1.0000\n",
            "Epoch 265/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5812 - accuracy: 1.0000\n",
            "Epoch 266/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5808 - accuracy: 1.0000\n",
            "Epoch 267/450\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5806 - accuracy: 1.0000\n",
            "Epoch 268/450\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5803 - accuracy: 1.0000\n",
            "Epoch 269/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5800 - accuracy: 1.0000\n",
            "Epoch 270/450\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5799 - accuracy: 1.0000\n",
            "Epoch 271/450\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5794 - accuracy: 1.0000\n",
            "Epoch 272/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5794 - accuracy: 1.0000\n",
            "Epoch 273/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5788 - accuracy: 1.0000\n",
            "Epoch 274/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5785 - accuracy: 1.0000\n",
            "Epoch 275/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5783 - accuracy: 1.0000\n",
            "Epoch 276/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5783 - accuracy: 1.0000\n",
            "Epoch 277/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5778 - accuracy: 1.0000\n",
            "Epoch 278/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5776 - accuracy: 1.0000\n",
            "Epoch 279/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5773 - accuracy: 1.0000\n",
            "Epoch 280/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5770 - accuracy: 1.0000\n",
            "Epoch 281/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5768 - accuracy: 1.0000\n",
            "Epoch 282/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5765 - accuracy: 1.0000\n",
            "Epoch 283/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5765 - accuracy: 1.0000\n",
            "Epoch 284/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5761 - accuracy: 1.0000\n",
            "Epoch 285/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5762 - accuracy: 1.0000\n",
            "Epoch 286/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5757 - accuracy: 1.0000\n",
            "Epoch 287/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5755 - accuracy: 1.0000\n",
            "Epoch 288/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5755 - accuracy: 1.0000\n",
            "Epoch 289/450\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5754 - accuracy: 1.0000\n",
            "Epoch 290/450\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5752 - accuracy: 1.0000\n",
            "Epoch 291/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5750 - accuracy: 1.0000\n",
            "Epoch 292/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5749 - accuracy: 1.0000\n",
            "Epoch 293/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5746 - accuracy: 1.0000\n",
            "Epoch 294/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5746 - accuracy: 1.0000\n",
            "Epoch 295/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5742 - accuracy: 1.0000\n",
            "Epoch 296/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5743 - accuracy: 1.0000\n",
            "Epoch 297/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5738 - accuracy: 1.0000\n",
            "Epoch 298/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5737 - accuracy: 1.0000\n",
            "Epoch 299/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5735 - accuracy: 1.0000\n",
            "Epoch 300/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5736 - accuracy: 1.0000\n",
            "Epoch 301/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5732 - accuracy: 1.0000\n",
            "Epoch 302/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5732 - accuracy: 1.0000\n",
            "Epoch 303/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5729 - accuracy: 1.0000\n",
            "Epoch 304/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5728 - accuracy: 1.0000\n",
            "Epoch 305/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5726 - accuracy: 1.0000\n",
            "Epoch 306/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5723 - accuracy: 1.0000\n",
            "Epoch 307/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5723 - accuracy: 1.0000\n",
            "Epoch 308/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5719 - accuracy: 1.0000\n",
            "Epoch 309/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5721 - accuracy: 1.0000\n",
            "Epoch 310/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5715 - accuracy: 1.0000\n",
            "Epoch 311/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5714 - accuracy: 1.0000\n",
            "Epoch 312/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5712 - accuracy: 1.0000\n",
            "Epoch 313/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5713 - accuracy: 1.0000\n",
            "Epoch 314/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5709 - accuracy: 1.0000\n",
            "Epoch 315/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5709 - accuracy: 1.0000\n",
            "Epoch 316/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5706 - accuracy: 1.0000\n",
            "Epoch 317/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5705 - accuracy: 1.0000\n",
            "Epoch 318/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5703 - accuracy: 1.0000\n",
            "Epoch 319/450\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5701 - accuracy: 1.0000\n",
            "Epoch 320/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5700 - accuracy: 1.0000\n",
            "Epoch 321/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5696 - accuracy: 1.0000\n",
            "Epoch 322/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5698 - accuracy: 1.0000\n",
            "Epoch 323/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5693 - accuracy: 1.0000\n",
            "Epoch 324/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5694 - accuracy: 1.0000\n",
            "Epoch 325/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5690 - accuracy: 1.0000\n",
            "Epoch 326/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5690 - accuracy: 1.0000\n",
            "Epoch 327/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5687 - accuracy: 1.0000\n",
            "Epoch 328/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5685 - accuracy: 1.0000\n",
            "Epoch 329/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5684 - accuracy: 1.0000\n",
            "Epoch 330/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5681 - accuracy: 1.0000\n",
            "Epoch 331/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5681 - accuracy: 1.0000\n",
            "Epoch 332/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5677 - accuracy: 1.0000\n",
            "Epoch 333/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5678 - accuracy: 1.0000\n",
            "Epoch 334/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5673 - accuracy: 1.0000\n",
            "Epoch 335/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5671 - accuracy: 1.0000\n",
            "Epoch 336/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5670 - accuracy: 1.0000\n",
            "Epoch 337/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5670 - accuracy: 1.0000\n",
            "Epoch 338/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5667 - accuracy: 1.0000\n",
            "Epoch 339/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5666 - accuracy: 1.0000\n",
            "Epoch 340/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5664 - accuracy: 1.0000\n",
            "Epoch 341/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5661 - accuracy: 1.0000\n",
            "Epoch 342/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5661 - accuracy: 1.0000\n",
            "Epoch 343/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5657 - accuracy: 1.0000\n",
            "Epoch 344/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5658 - accuracy: 1.0000\n",
            "Epoch 345/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5653 - accuracy: 1.0000\n",
            "Epoch 346/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5655 - accuracy: 1.0000\n",
            "Epoch 347/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5649 - accuracy: 1.0000\n",
            "Epoch 348/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5652 - accuracy: 1.0000\n",
            "Epoch 349/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5646 - accuracy: 1.0000\n",
            "Epoch 350/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5647 - accuracy: 1.0000\n",
            "Epoch 351/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5643 - accuracy: 1.0000\n",
            "Epoch 352/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5643 - accuracy: 1.0000\n",
            "Epoch 353/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5640 - accuracy: 1.0000\n",
            "Epoch 354/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5638 - accuracy: 1.0000\n",
            "Epoch 355/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5638 - accuracy: 1.0000\n",
            "Epoch 356/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5634 - accuracy: 1.0000\n",
            "Epoch 357/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5635 - accuracy: 1.0000\n",
            "Epoch 358/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5630 - accuracy: 1.0000\n",
            "Epoch 359/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5628 - accuracy: 1.0000\n",
            "Epoch 360/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5626 - accuracy: 1.0000\n",
            "Epoch 361/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5627 - accuracy: 1.0000\n",
            "Epoch 362/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5623 - accuracy: 1.0000\n",
            "Epoch 363/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5623 - accuracy: 1.0000\n",
            "Epoch 364/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5620 - accuracy: 1.0000\n",
            "Epoch 365/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5619 - accuracy: 1.0000\n",
            "Epoch 366/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5617 - accuracy: 1.0000\n",
            "Epoch 367/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5614 - accuracy: 1.0000\n",
            "Epoch 368/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5614 - accuracy: 1.0000\n",
            "Epoch 369/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5610 - accuracy: 1.0000\n",
            "Epoch 370/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5611 - accuracy: 1.0000\n",
            "Epoch 371/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5606 - accuracy: 1.0000\n",
            "Epoch 372/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5607 - accuracy: 1.0000\n",
            "Epoch 373/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5603 - accuracy: 1.0000\n",
            "Epoch 374/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5603 - accuracy: 1.0000\n",
            "Epoch 375/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5600 - accuracy: 1.0000\n",
            "Epoch 376/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5598 - accuracy: 1.0000\n",
            "Epoch 377/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5597 - accuracy: 1.0000\n",
            "Epoch 378/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5594 - accuracy: 1.0000\n",
            "Epoch 379/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5594 - accuracy: 1.0000\n",
            "Epoch 380/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5589 - accuracy: 1.0000\n",
            "Epoch 381/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5591 - accuracy: 1.0000\n",
            "Epoch 382/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5585 - accuracy: 1.0000\n",
            "Epoch 383/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5584 - accuracy: 1.0000\n",
            "Epoch 384/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5582 - accuracy: 1.0000\n",
            "Epoch 385/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5583 - accuracy: 1.0000\n",
            "Epoch 386/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5579 - accuracy: 1.0000\n",
            "Epoch 387/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5579 - accuracy: 1.0000\n",
            "Epoch 388/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5576 - accuracy: 1.0000\n",
            "Epoch 389/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5577 - accuracy: 1.0000\n",
            "Epoch 390/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5574 - accuracy: 1.0000\n",
            "Epoch 391/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5570 - accuracy: 1.0000\n",
            "Epoch 392/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5571 - accuracy: 1.0000\n",
            "Epoch 393/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5567 - accuracy: 1.0000\n",
            "Epoch 394/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5570 - accuracy: 1.0000\n",
            "Epoch 395/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5564 - accuracy: 1.0000\n",
            "Epoch 396/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5565 - accuracy: 1.0000\n",
            "Epoch 397/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5561 - accuracy: 1.0000\n",
            "Epoch 398/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5562 - accuracy: 1.0000\n",
            "Epoch 399/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5560 - accuracy: 1.0000\n",
            "Epoch 400/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5557 - accuracy: 1.0000\n",
            "Epoch 401/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5555 - accuracy: 1.0000\n",
            "Epoch 402/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5555 - accuracy: 1.0000\n",
            "Epoch 403/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5553 - accuracy: 1.0000\n",
            "Epoch 404/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5548 - accuracy: 1.0000\n",
            "Epoch 405/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5550 - accuracy: 1.0000\n",
            "Epoch 406/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5545 - accuracy: 1.0000\n",
            "Epoch 407/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5546 - accuracy: 1.0000\n",
            "Epoch 408/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5543 - accuracy: 1.0000\n",
            "Epoch 409/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5543 - accuracy: 1.0000\n",
            "Epoch 410/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5540 - accuracy: 1.0000\n",
            "Epoch 411/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5540 - accuracy: 1.0000\n",
            "Epoch 412/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5539 - accuracy: 1.0000\n",
            "Epoch 413/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5535 - accuracy: 1.0000\n",
            "Epoch 414/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5534 - accuracy: 1.0000\n",
            "Epoch 415/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5533 - accuracy: 1.0000\n",
            "Epoch 416/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5532 - accuracy: 1.0000\n",
            "Epoch 417/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5526 - accuracy: 1.0000\n",
            "Epoch 418/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5529 - accuracy: 1.0000\n",
            "Epoch 419/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5524 - accuracy: 1.0000\n",
            "Epoch 420/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5527 - accuracy: 1.0000\n",
            "Epoch 421/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5522 - accuracy: 1.0000\n",
            "Epoch 422/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5521 - accuracy: 1.0000\n",
            "Epoch 423/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5518 - accuracy: 1.0000\n",
            "Epoch 424/450\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5517 - accuracy: 1.0000\n",
            "Epoch 425/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5517 - accuracy: 1.0000\n",
            "Epoch 426/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5513 - accuracy: 1.0000\n",
            "Epoch 427/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5513 - accuracy: 1.0000\n",
            "Epoch 428/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5511 - accuracy: 1.0000\n",
            "Epoch 429/450\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5511 - accuracy: 1.0000\n",
            "Epoch 430/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5504 - accuracy: 1.0000\n",
            "Epoch 431/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5506 - accuracy: 1.0000\n",
            "Epoch 432/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5503 - accuracy: 1.0000\n",
            "Epoch 433/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5503 - accuracy: 1.0000\n",
            "Epoch 434/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5501 - accuracy: 1.0000\n",
            "Epoch 435/450\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5500 - accuracy: 1.0000\n",
            "Epoch 436/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5497 - accuracy: 1.0000\n",
            "Epoch 437/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5495 - accuracy: 1.0000\n",
            "Epoch 438/450\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5496 - accuracy: 1.0000\n",
            "Epoch 439/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5491 - accuracy: 1.0000\n",
            "Epoch 440/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5491 - accuracy: 1.0000\n",
            "Epoch 441/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5487 - accuracy: 1.0000\n",
            "Epoch 442/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5486 - accuracy: 1.0000\n",
            "Epoch 443/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5485 - accuracy: 1.0000\n",
            "Epoch 444/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5485 - accuracy: 1.0000\n",
            "Epoch 445/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5481 - accuracy: 1.0000\n",
            "Epoch 446/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5481 - accuracy: 1.0000\n",
            "Epoch 447/450\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5479 - accuracy: 1.0000\n",
            "Epoch 448/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5478 - accuracy: 1.0000\n",
            "Epoch 449/450\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5476 - accuracy: 1.0000\n",
            "Epoch 450/450\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5472 - accuracy: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78de6c1b9cc0>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# set up the basis for a feed forward network\n",
        "model = Sequential()\n",
        "\n",
        "# same X and y as above\n",
        "X = np.array([[0,0,1],\n",
        "                [0,1,1],\n",
        "                [1,0,1],\n",
        "                [1,1,1] ])\n",
        "y = np.array([[0,1,1,0]]).T\n",
        "\n",
        "# hidden layer\n",
        "# you can play around with different activation functions\n",
        "# W was (input_features, hidden_units)\n",
        "model.add(Dense(units=4, activation='relu', input_dim=X.shape[1]))\n",
        "\n",
        "# output layer\n",
        "# activation function is our classification function\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# configure the learning process\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# 1 epoch = once through the data\n",
        "model.fit(X, y, epochs=450, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJReN3eTaQsM",
        "outputId": "594538d1-ae72-440c-d7b5-386d12ee7142"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x78de6c1d8c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 37ms/step\n",
            "Assigned probabilities: [[0.34999427]\n",
            " [0.5415805 ]\n",
            " [0.5416816 ]\n",
            " [0.4129878 ]]\n",
            "Assigned labels: [0, 1, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "x_test = np.array([[0,0,1],\n",
        "                [0,1,1],\n",
        "                [1,0,1],\n",
        "                [1,1,1] ])\n",
        "y_test = np.array([[0,1,1,0]]).T\n",
        "labels = model.predict(x_test)\n",
        "print(\"Assigned probabilities:\", labels)\n",
        "print(\"Assigned labels:\", [1 if y_hat_val > .5 else 0 for y_hat_val in labels])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfafug5RaQsM"
      },
      "source": [
        "6. How many epochs did you need for 100% accuracy? __We trained the model for 450 Epochs but we gained 100% accuracy at the 93rd Epoch__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhalDgFFaQsM"
      },
      "source": [
        "Interested in getting deeper into neural nets?\n",
        "\n",
        "\n",
        "Here are two places to start from:\n",
        "- take a look at the data that you can load from [`nltk`](https://www.nltk.org/data.html) and [`scikit-learn`](https://scikit-learn.org/stable/datasets/index.html#dataset-loading-utilities), then work on creating a neural net to do either binary or multinomial classification\n",
        "- take a look at the tensorflow + keras word embeddings tutorial [here](https://www.tensorflow.org/tutorials/text/word_embeddings). Note! This tutorial mentions RNNs, which are special kind of neural net (they are not the feedforward architecture that we've seen so far). We'll get into RNNs after next week."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
