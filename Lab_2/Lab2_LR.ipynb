{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab 2: Simple Logistic Regression\n",
    "===============\n",
    "\n",
    "2/14/24, CS 6120 Natural Language Processing\\\n",
    "Complete in groups of up to 3.\\\n",
    "Please indicate your partners on the Gradescope submission. Due Fri 2/16, 11:59 PM.\n",
    "\n",
    "------------------\n",
    "Courtesy of Felix Muzny\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 0: Group Member Names:\n",
    "-----------------\n",
    "1. Nikita Mandal\n",
    "2. Bhuvan Channagiri\n",
    "3. Vaibhav Kejriwal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Logistic Regression\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # helpful for dot products\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the sigmoid function!\n",
    "# access the value of e either via math.e or np.e\n",
    "# sigmoid(z) = 1 / (1 + e^-z)\n",
    "\n",
    "def sigmoid(z: float) -> float:\n",
    "    return 1/(1+math.exp(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x, w, b\n",
    "x = [3, 2, 1, 3, 0, 4.15]\n",
    "w = [2.5, -5, -1.2, 0.5, 2, 0.7]\n",
    "b = 0.1  # initialize to some value\n",
    "y = 1  # true label of this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(y = 1): 0.7310585786300049\n",
      "P(y = 0): 0.2689414213699951\n",
      "Gradients for weights: [-0.8068242641099853, -0.5378828427399902, -0.2689414213699951, -0.8068242641099853, -0.0, -1.1161068986854799]\n",
      "Gradient for bias term: -0.2689414213699951\n",
      "Updated weights: [6.776168599782922, -2.1492209334780514, 0.2253895332609741, 4.776168599782922, 2.0, 6.615366563033044]\n",
      "Updated bias term: 1.525389533260974\n",
      "Updated P(y = 1): 0.7310585786300049\n",
      "Updated P(y = 0): 0.2689414213699951\n"
     ]
    }
   ],
   "source": [
    "# TODO: implement one update from SGD\n",
    "# print the intermediate values as you go\n",
    "\n",
    "# 1. calculate P(y = 1) and P(y = 0), print out their values\n",
    "\n",
    "\n",
    "\n",
    "# 2. calculate your gradients for each weight, print them out\n",
    "\n",
    "\n",
    "# 3. calculate your updated weights, print them out\n",
    "learning_rate = 1\n",
    "\n",
    "\n",
    "# 4. Check how you did! Calculate P(y = 1) and P(y = 0) again\n",
    "\n",
    "\n",
    "# 5. test out the effects of changing the value of your learning rate \n",
    "# What about also updating your bias term?\n",
    "\n",
    "\n",
    "# YOUR ANSWERS HERE\n",
    "# 1. Calculate P(y = 1) and P(y = 0)\n",
    "z = sum([xi * wi for xi, wi in zip(x, w)]) + b\n",
    "p_y1 = sigmoid(z)\n",
    "p_y0 = 1 - p_y1\n",
    "print(\"P(y = 1):\", p_y1)\n",
    "print(\"P(y = 0):\", p_y0)\n",
    "\n",
    "# 2. Calculate gradients for each weight\n",
    "grad_w = [(p_y1 - y) * xi for xi in x]\n",
    "grad_b = p_y1 - y\n",
    "print(\"Gradients for weights:\", grad_w)\n",
    "print(\"Gradient for bias term:\", grad_b)\n",
    "\n",
    "# 3. Calculate updated weights\n",
    "learning_rate = 0.3\n",
    "w = [wi - learning_rate * grad_wi for wi, grad_wi in zip(w, grad_w)]\n",
    "b -= learning_rate * grad_b\n",
    "print(\"Updated weights:\", w)\n",
    "print(\"Updated bias term:\", b)\n",
    "\n",
    "# 4. Check how you did! Calculate P(y = 1) and P(y = 0) again\n",
    "z = sum([xi * wi for xi, wi in zip(x, w)]) + b\n",
    "p_y1 = sigmoid(z)\n",
    "p_y0 = 1 - p_y1\n",
    "print(\"Updated P(y = 1):\", p_y1)\n",
    "print(\"Updated P(y = 0):\", p_y0)\n",
    "\n",
    "# 5. Test out the effects of changing the value of your learning rate\n",
    "# What about also updating your bias term?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
