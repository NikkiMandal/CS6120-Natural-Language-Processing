{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3ab45d-c79f-465d-8041-78bf2a58173d",
   "metadata": {
    "id": "7c3ab45d-c79f-465d-8041-78bf2a58173d"
   },
   "source": [
    "Task 4: Train an LSTM Model (40 points)\n",
    "----\n",
    "1. Using PyTorch, implement a neural network that uses one or more LSTM cells to do sentiment analysis. Use the nn.Embedding, nn.LSTM and nn.Linear layers to construct your model.\n",
    "2. Note that sequence processing works differently with the PyTorch Embedding layer as compared to my sample code from class. The model input expects a padded tensor of token indices from the vocabulary, instead of one-hot encodings. For evaluation, use a vocabulary size of 10000 (max_features = 10000).\n",
    "3. The model should have a single output with the sigmoid activation function for classification. The dimensions of the embedding layer and the hidden layer(s) are up to you, but please make sure your model does not take more than ~3 minutes to train.\n",
    "4. Evaluate the model using PyTorch functions for average accuracy, area under the ROC curve and F1 scores (see [torchedev](https://pytorch.org/torcheval/stable/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04554ea0-360d-43cd-b5d4-67442da6dbb6",
   "metadata": {
    "id": "04554ea0-360d-43cd-b5d4-67442da6dbb6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from nltk.corpus import stopwords\n",
    "    from collections import Counter\n",
    "    import string\n",
    "    import re\n",
    "    import seaborn as sns\n",
    "    from tqdm import tqdm\n",
    "    import matplotlib.pyplot as plt\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b01e664a-e80a-4129-968c-3a3df25617a5",
   "metadata": {
    "id": "b01e664a-e80a-4129-968c-3a3df25617a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d028c44-0bcc-47a1-b9fe-78d1ddc030f2",
   "metadata": {
    "id": "2d028c44-0bcc-47a1-b9fe-78d1ddc030f2"
   },
   "outputs": [],
   "source": [
    "train_data_file = 'movie_reviews_train.txt'\n",
    "train_df = pd.read_csv(train_data_file, sep='\\t', header=None, names=['id', 'review', 'label'])[['review', 'label']]\n",
    "X_train, y_train = train_df['review'].values, train_df['label'].values\n",
    "\n",
    "dev_data_file = 'movie_reviews_dev.txt'\n",
    "dev_df = pd.read_csv(dev_data_file, sep='\\t', header=None, names=['id', 'review', 'label'])[['review', 'label']]\n",
    "X_dev, y_dev = dev_df['review'].values, dev_df['label'].values\n",
    "\n",
    "test_data_file = 'movie_reviews_test.txt'\n",
    "test_df = pd.read_csv(test_data_file, sep='\\t', header=None, names=['id', 'review', 'label'])[['review', 'label']]\n",
    "X_test, y_test = test_df['review'].values, test_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa0d2b64-25f0-4bdd-a6ff-8a8aea1c586b",
   "metadata": {
    "id": "fa0d2b64-25f0-4bdd-a6ff-8a8aea1c586b"
   },
   "outputs": [],
   "source": [
    "def preprocess_token(s): # This function is for pre-processing each token, not the entire sequence\n",
    "    # Retain only alphanumeric characters\n",
    "    s = re.sub(r'[^a-zA-Z0-9]', '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r'\\d', '', s)\n",
    "    # Replace all whitespace sequences with no space\n",
    "    s = re.sub(r'\\s+', '', s)\n",
    "    return s\n",
    "\n",
    "def tokenize(x_train, x_dev, x_test, vocab_size): # This function is for pre-processing strings, which uses the above.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    all_tokens = []\n",
    "    for text in np.concatenate((x_train, x_dev, x_test)):\n",
    "        for word in text.split():\n",
    "            if word.lower() not in stop_words:# and preprocess_token(word) != '':\n",
    "                all_tokens.append(preprocess_token(word))\n",
    "                \n",
    "    word_counts = Counter(all_tokens)              \n",
    "    vocab = Counter()\n",
    "    vocab = {word: idx + 1 for idx, (word, _) in enumerate(word_counts.most_common(vocab_size-1))}\n",
    "    vocab[\"<PAD>\"] = 0\n",
    "    \n",
    "    x_train_seq = []\n",
    "    x_dev_seq = []\n",
    "    x_test_seq = []\n",
    "\n",
    "    for doc in x_train:\n",
    "        tokens = [preprocess_token(word.lower()) for word in doc.split() if word.lower() not in stop_words]\n",
    "        sequence = [vocab.get(token) for token in tokens if token in vocab]\n",
    "        x_train_seq.append(sequence)\n",
    "#         pass\n",
    "\n",
    "    for doc in x_dev:\n",
    "        tokens = [preprocess_token(word.lower()) for word in doc.split() if word.lower() not in stop_words]\n",
    "        sequence = [vocab.get(token) for token in tokens if token in vocab]\n",
    "        x_dev_seq.append(sequence)\n",
    "\n",
    "    for doc in x_test:\n",
    "        tokens = [preprocess_token(word.lower()) for word in doc.split() if word.lower() not in stop_words]\n",
    "        sequence = [vocab.get(token) for token in tokens if token in vocab]\n",
    "        x_test_seq.append(sequence)\n",
    "        \n",
    "    max_len = max(max(len(seq) for seq in sequences) for sequences in [x_train_seq, x_dev_seq, x_test_seq])\n",
    "\n",
    "    x_train_seq = [[0]*(max_len-len(seq)) + seq for seq in x_train_seq]\n",
    "    x_dev_seq = [[0]*(max_len-len(seq)) + seq for seq in x_dev_seq]\n",
    "    x_test_seq = [[0]*(max_len-len(seq)) + seq for seq in x_test_seq]\n",
    "\n",
    "    return x_train_seq,x_dev_seq,x_test_seq,vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "912fd19f-04b5-4549-b944-142b99be21f7",
   "metadata": {
    "id": "912fd19f-04b5-4549-b944-142b99be21f7"
   },
   "outputs": [],
   "source": [
    "# Tokenize your train, test and development data\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "train_padded, dev_padded, test_padded, vocab = tokenize(X_train, X_dev, X_test, vocab_size=10000)\n",
    "train_padded = torch.tensor(train_padded, dtype=torch.long)\n",
    "dev_padded = torch.tensor(dev_padded, dtype=torch.long)\n",
    "test_padded = torch.tensor(test_padded, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_dev = torch.tensor(y_dev, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359dce7f-04d1-4a21-a6d7-0297b7dd1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_padded, y_train)\n",
    "dev_data = TensorDataset(dev_padded, y_dev)\n",
    "test_data = TensorDataset(test_padded, y_test)\n",
    "\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dea3832-6ad4-4c9c-845b-bfbaa04dc03a",
   "metadata": {
    "id": "0dea3832-6ad4-4c9c-845b-bfbaa04dc03a"
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self,num_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        super(SentimentRNN,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.no_layers = num_layers\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "\n",
    "        ###### YOUR CODE HERE #######\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=drop_prob)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        # Linear and sigmoid layer\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self,x,hidden):\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        y_pred = self.sigmoid(out)\n",
    "        y_pred = y_pred.view(batch_size, -1)\n",
    " \n",
    "        y_pred = y_pred[:, -1]\n",
    "        return y_pred, hidden        \n",
    "\n",
    "#         pass\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76fbf614-ddc3-475a-a488-a8dddad7282b",
   "metadata": {
    "id": "76fbf614-ddc3-475a-a488-a8dddad7282b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(10000, 64)\n",
      "  (lstm): LSTM(64, 256, num_layers=4, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "no_layers = 4       #tried increasing layers to see change in accuracy\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 64      # Increased embedding dimension for trial\n",
    "output_dim = 1\n",
    "hidden_dim = 256        # Increased hidden dimension for trial\n",
    "\n",
    "model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8d96d5d-aa54-41e0-856f-03cd7a740029",
   "metadata": {
    "id": "b8d96d5d-aa54-41e0-856f-03cd7a740029"
   },
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "\n",
    "# you should use binary cross-entropy as your loss function and Adam optimizer for this task\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.BCELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr) \n",
    "\n",
    "\n",
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "\n",
    "    # pass\n",
    "    pred_rounded = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred_rounded==label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b226e1f-3db2-4c7e-abe4-e3a1b2c901e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss : 0.694859741255641 val_loss : 0.6926769614219666\n",
      "train_accuracy : 0.480625 val_accuracy : 0.57\n",
      "Validation loss decreased (inf --> 0.69268).  Saving model ...\n",
      "====================================================================================================\n",
      "Epoch 2\n",
      "train_loss : 0.6832661777734756 val_loss : 0.6868956238031387\n",
      "train_accuracy : 0.578125 val_accuracy : 0.5\n",
      "Validation loss decreased (0.69268 --> 0.68690).  Saving model ...\n",
      "====================================================================================================\n",
      "Epoch 3\n",
      "train_loss : 0.6244987193495035 val_loss : 0.724558025598526\n",
      "train_accuracy : 0.658125 val_accuracy : 0.615\n",
      "====================================================================================================\n",
      "Epoch 4\n",
      "train_loss : 0.5160010028630495 val_loss : 0.7529969215393066\n",
      "train_accuracy : 0.759375 val_accuracy : 0.61\n",
      "====================================================================================================\n",
      "Epoch 5\n",
      "train_loss : 0.40143234468996525 val_loss : 0.8260402530431747\n",
      "train_accuracy : 0.825625 val_accuracy : 0.62\n",
      "====================================================================================================\n",
      "Epoch 6\n",
      "train_loss : 0.30122075136750937 val_loss : 0.9024200737476349\n",
      "train_accuracy : 0.88125 val_accuracy : 0.635\n",
      "====================================================================================================\n",
      "Epoch 7\n",
      "train_loss : 0.1987796991597861 val_loss : 1.0874906182289124\n",
      "train_accuracy : 0.93875 val_accuracy : 0.605\n",
      "====================================================================================================\n",
      "Epoch 8\n",
      "train_loss : 0.1543990708887577 val_loss : 1.1610200703144073\n",
      "train_accuracy : 0.94625 val_accuracy : 0.625\n",
      "====================================================================================================\n",
      "Epoch 9\n",
      "train_loss : 0.08968792132509407 val_loss : 1.2661308497190475\n",
      "train_accuracy : 0.971875 val_accuracy : 0.62\n",
      "====================================================================================================\n",
      "Epoch 10\n",
      "train_loss : 0.056610241561429575 val_loss : 1.790366530418396\n",
      "train_accuracy : 0.9825 val_accuracy : 0.635\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "clip = 5\n",
    "epochs = 10\n",
    "valid_loss_min = np.Inf\n",
    "epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses, train_acc = [], 0.0\n",
    "    model.train()\n",
    "    h = model.init_hidden(batch_size)\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)   \n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output,h = model(inputs,h)\n",
    "        \n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        accuracy = acc(output,labels)\n",
    "        train_acc += accuracy\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    val_losses, val_acc = [], 0.0\n",
    "    model.eval()\n",
    "    for inputs, labels in dev_loader:\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            output, val_h = model(inputs, val_h)\n",
    "            val_loss = criterion(output.squeeze(), labels.float())\n",
    "            val_losses.append(val_loss.item())  \n",
    "            accuracy = acc(output,labels)\n",
    "            val_acc += accuracy\n",
    "            \n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc/len(dev_loader.dataset)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    print(f'Epoch {epoch+1}') \n",
    "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "    print(f'train_accuracy : {epoch_train_acc} val_accuracy : {epoch_val_acc}')\n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        print('Validation loss decreased ({:.5f} --> {:.5f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "        valid_loss_min = epoch_val_loss\n",
    "    print(50 * '==')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa883eea",
   "metadata": {},
   "source": [
    "NOTE: your train loss should be smaller than 1 and your train accuracy should be over 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb78165f-b013-4631-903d-c10d3e0958bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy is: 62.0%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_h = model.init_hidden(batch_size)\n",
    "test_acc = 0.0\n",
    "\n",
    "# Evaluate model on your test data and report the accuracy\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "model.eval()\n",
    "test_acc = 0.0\n",
    "test_h = model.init_hidden(batch_size)\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "    \n",
    "    output, test_h = model(inputs, test_h)\n",
    "    accuracy = acc(output, labels)\n",
    "    test_acc += accuracy\n",
    "\n",
    "test_accuracy = test_acc / len(test_loader.dataset)\n",
    "print(f'Test Accuracy is: {test_accuracy * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332ea53",
   "metadata": {},
   "source": [
    "NOTE: your eval accuracy should be of at least 60%."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
