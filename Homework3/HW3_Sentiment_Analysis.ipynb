{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28f7c45-f3f5-45b8-8b41-5012bc57d4bd",
   "metadata": {
    "id": "e28f7c45-f3f5-45b8-8b41-5012bc57d4bd"
   },
   "source": [
    "Homework 3: Sentiment Analysis\n",
    "----\n",
    "\n",
    "The following instructions apply to all notebooks and `.py` files you submit for this homework.\n",
    "\n",
    "Due date: April 15th, 2024 11:59 PM (EST)\n",
    "\n",
    "Total Points: (105)\n",
    "- Task 0: 05 points\n",
    "- Task 1: 10 points\n",
    "- Task 2: 20 points\n",
    "- Task 3: 25 points\n",
    "- Task 4: 40 points (question in LSTM_EncDec.ipynb)\n",
    "\n",
    "Goals:\n",
    "- understand the difficulties of counting and probabilities in NLP applications\n",
    "- work with real world data using different approaches to classification\n",
    "- stress test your model (to some extent)\n",
    "\n",
    "\n",
    "Allowed python modules:\n",
    "- `numpy`, `matplotlib`, `keras`, `pytorch`, `nltk`, `pandas`, `sci-kit learn` (`sklearn`), `seaborn`, and all built-in python libraries (e.g. `math` and `string`)\n",
    "- if you would like to use a library not on this list, please check with us on Campuswire first.\n",
    "- all *necessary* imports have been included for you (all imports that we used in our solution)\n",
    "\n",
    "Instructions:\n",
    "- Complete outlined problems in this notebook.\n",
    "- When you have finished, __clear the kernel__ and __run__ your notebook \"fresh\" from top to bottom. Ensure that there are __no errors__.\n",
    "    - If a problem asks for you to write code that does result in an error (as in, the answer to the problem is an error), leave the code in your notebook but commented out so that running from top to bottom does not result in any errors.\n",
    "- Double check that you have completed Task 0.\n",
    "- Submit your work on Gradescope.\n",
    "- Double check that your submission on Gradescope looks like you believe it should."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3f1aa-6b6e-4953-a30d-a79ab0794170",
   "metadata": {
    "id": "cfc3f1aa-6b6e-4953-a30d-a79ab0794170"
   },
   "source": [
    "Names & Sections\n",
    "----\n",
    "Names: __Nikita Vinod Mandal__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4307e0-20b7-4ed3-9fa0-086f899b4583",
   "metadata": {
    "id": "2a4307e0-20b7-4ed3-9fa0-086f899b4583"
   },
   "source": [
    "Task 0: Name, References, Reflection (5 points)\n",
    "---\n",
    "\n",
    "References\n",
    "---\n",
    "List the resources you consulted to complete this homework here. Write one sentence per resource about what it provided to you. If you consulted no references to complete your assignment, write a brief sentence stating that this is the case and why it was the case for you.\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "    - Sklearn's linear and logistic regression model\n",
    "- https://www.geeksforgeeks.org/counters-in-python-set-2-accessing-counters/\n",
    "    - mostcommon() function\n",
    "- https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Architecture/feedforward.html\n",
    "    - Feed Forward Neural network architecture\n",
    "- https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "    - Neural network implementation\n",
    "    \n",
    "\n",
    "AI Collaboration\n",
    "---\n",
    "Following the *Policy on the use of Generative AI* in the syllabus, please cite any LLMs that you used here and briefly describe what you used them for, including to improve language clarity in the written sections.\n",
    "\n",
    "I used ChatGPT to debug a few cases. For some time I was getting the same vocabulary size for both custom vectorization and SKlearn. So I used ChatGPTthe possible causes for the same and the solutions that can be implemented.\n",
    "\n",
    "Reflection\n",
    "----\n",
    "Answer the following questions __after__ you complete this assignment (no more than 1 sentence per question required, this section is graded on completion):\n",
    "\n",
    "1. Does this work reflect your best effort? I tried my best. Have to learn more.\n",
    "2. What was/were the most challenging part(s) of the assignment? Feedforward Neural Network\n",
    "3. If you want feedback, what function(s) or problem(s) would you like feedback on and why? Feedforward Neural Network. I also want feedback on the RNN network since I got less accuracy.\n",
    "4. Briefly reflect on how your partnership functioned--who did which tasks, how was the workload on each of you individually as compared to the previous homeworks, etc. N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c4773-456b-4d5a-bdce-df7bbcb3d16b",
   "metadata": {
    "id": "669c4773-456b-4d5a-bdce-df7bbcb3d16b"
   },
   "source": [
    "Task 1: Provided Data Write-Up (10 points)\n",
    "---\n",
    "\n",
    "Every time you use a data set in an NLP application (or in any software application), you should be able to answer a set of questions about that data. Answer these now. Default to no more than 1 sentence per question needed. If more explanation is necessary, do give it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281567d-b99b-45df-8b1b-2b0e35cb9196",
   "metadata": {
    "id": "c281567d-b99b-45df-8b1b-2b0e35cb9196"
   },
   "source": [
    "This is about the __provided__ movie review data set.\n",
    "\n",
    "1. Where did you get the data from? The provided dataset(s) were sub-sampled from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "2. (1 pt) How was the data collected (where did the people acquiring the data get it from and how)? \n",
    "\n",
    "    - Collected from IMDB. Ratings > 7 are positive and ratings < 5 are negative.\n",
    "\n",
    "3. (2 pts) How large is the dataset (answer for both the train and the dev set, separately)? (# reviews, # tokens in both the train and dev sets)\n",
    "\n",
    "    - #reviews: 25,000 train set and 25,000 test set.\n",
    "    - #tokens: 369127 in train set and 47155 in test set.\n",
    "\n",
    "4. (1 pt) What is your data? (i.e. newswire, tweets, books, blogs, etc)\n",
    "\n",
    "    - Movie reviews\n",
    "\n",
    "5. (1 pt) Who produced the data? (who were the authors of the text? Your answer might be a specific person or a particular group of people)\n",
    "\n",
    "    - Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher\n",
    "\n",
    "6. (2 pts) What is the distribution of labels in the data (answer for both the train and the dev set, separately)?\n",
    "\n",
    "    - Two classes - positive and negative\n",
    "\n",
    "7. (2 pts) How large is the vocabulary (answer for both the train and the dev set, separately)?\n",
    "    - 47698 in train set and 11713 in test set (vocab count)\n",
    "\n",
    "\n",
    "8. (1 pt) How big is the overlap between the vocabulary for the train and dev set?\n",
    "    - 7245 (vocab_train intersection vocab_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64fd8f-f130-4f1b-8624-c151c502344b",
   "metadata": {
    "id": "2d64fd8f-f130-4f1b-8624-c151c502344b"
   },
   "source": [
    "Task 2: Train a Logistic Regression Model (20 points)\n",
    "----\n",
    "1. Implement a custom function to read in a dataset, and return a list of tuples, using the Tf-Idf feature extraction technique.\n",
    "2. Compare your implementation to `sklearn`'s TfidfVectorizer (imported below) by timing both on the provided datasets using the time module.\n",
    "3. Using each set of features, and `sklearn`'s implementation of `LogisticRegression`, train a machine learning model to predict sentiment on the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9bbb93-9a5d-4326-8dd0-d2ffcf242123",
   "metadata": {
    "id": "0e9bbb93-9a5d-4326-8dd0-d2ffcf242123"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c515ae-3637-42b6-9412-e0710e7821bb",
   "metadata": {
    "id": "62c515ae-3637-42b6-9412-e0710e7821bb"
   },
   "outputs": [],
   "source": [
    "# The following function reads a data-file and splits the contents by tabs.\n",
    "# The first column is an ID, and thus is discarded. The second column consists of the actual reviews data.\n",
    "# The third column is the true label for each data point.\n",
    "\n",
    "# The function returns two objects - a list of all reviews, and a numpy array of labels.\n",
    "# You will need to use this function later.\n",
    "\n",
    "def get_lists(input_file):\n",
    "    f=open(input_file, 'r')\n",
    "    lines = [line.split('\\t')[1:] for line in f.readlines()]\n",
    "    X = [row[0] for row in lines]\n",
    "    y=np.array([int(row[1]) for row in lines])\n",
    "    return X, y\n",
    "\n",
    "# Fill in the following function to take a corpus (list of reviews) as input,\n",
    "# extract TfIdf values and return an array of features and the vocabulary.\n",
    "\n",
    "# If the vocabulary argument is supplied, then the function should only convert the input corpus\n",
    "# to feature vectors using the provided vocabulary and the max_features argument (if not None).\n",
    "# In this case, the function should return feature vectors and the supplied vocabulary.\n",
    "\n",
    "# If the max_features parameter is set to None, then all words in the corpus should be used.\n",
    "# If the max_features parameter is specified (say, k),\n",
    "# then only use the k most frequent words in the corpus to build your vocabulary.\n",
    "\n",
    "# The function should return two things.\n",
    "\n",
    "# The first object should be a numpy array of shape (n_documents, vocab_size),\n",
    "# which contains the TF-IDF feature vectors for each document.\n",
    "\n",
    "# The second object should be a dictionary of the words in the vocabulary,\n",
    "# mapped to their corresponding index in alphabetical sorted order.\n",
    "\n",
    "\n",
    "def get_tfidf_vectors(token_lists, max_features=None, vocabulary=None):\n",
    "    # Tokenization and vocabulary creation\n",
    "    if vocabulary is None:\n",
    "        # Tokenize documents, remove stopwords, and count token occurrences\n",
    "        all_tokens = [token for doc in token_lists for token in nltk.tokenize.word_tokenize(doc.lower()) if token not in stopwords]\n",
    "        token_counts = Counter(all_tokens)\n",
    "        \n",
    "        # Select most common tokens up to max_features limit\n",
    "        most_common_tokens = token_counts.most_common(max_features)\n",
    "        \n",
    "        # Create vocabulary from selected tokens\n",
    "        vocabulary = {word: idx for idx, (word, _) in enumerate(most_common_tokens)}\n",
    "\n",
    "    # Initialize TF-IDF matrix\n",
    "    n_docs = len(token_lists)\n",
    "    tfidf_matrix = np.zeros((n_docs, len(vocabulary)))\n",
    "    \n",
    "    # Compute document frequencies\n",
    "    df = Counter()\n",
    "    for doc in token_lists:\n",
    "        tokens = set(nltk.tokenize.word_tokenize(doc.lower())) \n",
    "        filtered_tokens = [token for token in tokens if token in vocabulary]\n",
    "        df.update(filtered_tokens)\n",
    "\n",
    "    # Compute TF-IDF values\n",
    "    for i, doc in enumerate(token_lists):\n",
    "        tokens = nltk.tokenize.word_tokenize(doc.lower())\n",
    "        filtered_tokens = [token for token in tokens if token in vocabulary]\n",
    "        term_freq = Counter(filtered_tokens)\n",
    "        doc_len = len(filtered_tokens)\n",
    "        for word, count in term_freq.items():\n",
    "            tf = count / doc_len  \n",
    "            idf = np.log((n_docs + 1) / (df[word] + 1)) + 1 \n",
    "            tfidf_matrix[i, vocabulary[word]] = tf * idf\n",
    "\n",
    "    # Normalize TF-IDF matrix\n",
    "    norms = np.sqrt((tfidf_matrix ** 2).sum(axis=1, keepdims=True))\n",
    "    tfidf_matrix = tfidf_matrix / norms\n",
    "\n",
    "    return tfidf_matrix, vocabulary\n",
    "\n",
    "# Function to calculate TF-IDF vectors using scikit-learn\n",
    "def get_tfidf_vectors_sk(token_lists, max_features=None, vocabulary=None):\n",
    "    if vocabulary is not None:\n",
    "        # Use provided vocabulary and max_features to create TF-IDF vectors\n",
    "        tfidf_vectorizer = TfidfVectorizer(vocabulary=vocabulary, stop_words=stopwords, max_features=max_features)\n",
    "    else:\n",
    "        # Create TF-IDF vectors from token lists using scikit-learn's TfidfVectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords, max_features=max_features)\n",
    "    \n",
    "    # Transform token lists into TF-IDF vectors\n",
    "    tfidf_vectors = tfidf_vectorizer.fit_transform(token_lists)\n",
    "    \n",
    "    vocab = tfidf_vectorizer.vocabulary_\n",
    "\n",
    "    return tfidf_vectors, vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62226f12-d0e5-4e74-887e-5cd852b96707",
   "metadata": {
    "id": "62226f12-d0e5-4e74-887e-5cd852b96707"
   },
   "source": [
    "We will now compare the runtime of our Tf-Idf implementation to the `sklearn` implementation. Call the respective functions with appropriate arguments in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e6c726-0e12-4e1f-8c8b-146a9a506a3f",
   "metadata": {
    "id": "e2e6c726-0e12-4e1f-8c8b-146a9a506a3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  9.877344131469727  seconds\n",
      "Time taken:  0.28160858154296875  seconds\n"
     ]
    }
   ],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "TEST_FILE = \"movie_reviews_test.txt\"\n",
    "\n",
    "train_corpus, y_train = get_lists(TRAIN_FILE)\n",
    "\n",
    "# First we will use our custom vectorizer to convert words to features, and time it.\n",
    "start = time.time()\n",
    "###### YOUR CODE HERE #######\n",
    "X_train_c , vocb = get_tfidf_vectors(train_corpus, max_features=None, vocabulary=None)\n",
    "end = time.time()\n",
    "print(\"Time taken: \", end-start, \" seconds\")\n",
    "\n",
    "# Next we will use sklearn's TfidfVectorizer to load in the data, and time it.\n",
    "\n",
    "###### YOUR CODE HERE #######\n",
    "start = time.time()\n",
    "X_train_sk , vocb_sk = get_tfidf_vectors_sk(train_corpus, max_features=None, vocabulary=None)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken: \", end-start, \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35afb9c6",
   "metadata": {},
   "source": [
    "NOTE: Ideally, your vectorizer should be within one order of magnitude of the sklearn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bdbf31c-bf6b-4ab4-b87b-47dd7c5468a8",
   "metadata": {
    "id": "8bdbf31c-bf6b-4ab4-b87b-47dd7c5468a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of my vocab:  27035\n",
      "Length of sklearn's vocab:  22460\n",
      "Sparsity of my custom features:  99.60588588866285\n",
      "Sparsity of sklearn's features:  99.562071460374\n"
     ]
    }
   ],
   "source": [
    "# Displaying vocabulary lengths\n",
    "print(\"Length of my vocab: \", len(vocb))\n",
    "print(\"Length of sklearn's vocab: \", len(vocb_sk))\n",
    "\n",
    "# Calculating sparsity of custom features\n",
    "zero_elements = np.count_nonzero(X_train_c) \n",
    "total_elements = X_train_c.shape[0] * X_train_c.shape[1]  \n",
    "sparsity = (1 - (zero_elements / total_elements)) * 100\n",
    "print(\"Sparsity of my custom features: \", sparsity)\n",
    "\n",
    "# Calculating sparsity of sklearn's features\n",
    "zero_elements = np.count_nonzero(X_train_sk.toarray()) \n",
    "total_elements = X_train_sk.shape[0] * X_train_sk.shape[1]  \n",
    "sparsity_sk = (1 - (zero_elements / total_elements)) * 100\n",
    "print(\"Sparsity of sklearn's features: \", sparsity_sk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463f022-7433-4397-88ed-eed8b01b1a45",
   "metadata": {
    "id": "6463f022-7433-4397-88ed-eed8b01b1a45"
   },
   "source": [
    "1. How large is the vocabulary generated by your vectorizer?  \n",
    "    - 27035\n",
    "2. How large is the vocabulary generated by the `sklearn` TfidfVectorizer?\n",
    "    - 22460\n",
    "3. Where might these differences be coming from?\n",
    "    - These differences could stem from variations in parameter settings, preprocessing steps, or tokenization strategies employed in both approaches.\n",
    "4. What steps did you take to ensure your vectorizer is optimized for best possible runtime?\n",
    "    -  Matrix multiplication was utilized to calculate TF-IDF instead of using explicit for loops, which can significantly enhance computational efficiency. Additionally, stop words were removed during preprocessing to streamline the feature extraction process and improve vectorization performance.\n",
    "5. How sparse are your custom features (average percentage of features per review that are zero)?\n",
    "    - 99.6% sparse\n",
    "6. How sparse are the TfidfVectorizer's features?\n",
    "    - 99.56% sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f496b",
   "metadata": {},
   "source": [
    "NOTE: if you set the lowercase option to False, the sklearn vectorizer should have a vocabulary of around 50k words/tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c76612-4dd4-489c-90e4-0c38029c32d1",
   "metadata": {
    "id": "09c76612-4dd4-489c-90e4-0c38029c32d1"
   },
   "source": [
    "**Logistic Regression**\n",
    "\n",
    "Now, we will compare how our custom features stack up against sklearn's TfidfVectorizer, by training two separate Logistic Regression classifiers - one on each set of feature vectors. Then load the test set, and convert it to two sets of feature vectors, one using our custom vectorizer (to do this, provide the vocabulary as a function argument), and one using sklearn's Tfidf (use the same object as before to transform the test inputs). For both classifiers, print the average accuracy on the test set and the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68eb6d36-1e71-4665-aab4-1217a42e6dcc",
   "metadata": {
    "id": "68eb6d36-1e71-4665-aab4-1217a42e6dcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy using features extracted from custom vectorizer: 80.0%\n",
      "F1 Score using features extracted from custom vectorizer: 80.769%\n",
      "Average Accuracy using features extracted from sklearn's Tfidfvectorizer: 81.5%\n",
      "F1 Score using features extracted from sklearn's Tfidfvectorizer: 82.629%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Train Logistic Regression model using custom feature vectors\n",
    "logisticReg_model_custom = LogisticRegression()\n",
    "logisticReg_model_custom.fit(X_train_c, y_train)\n",
    "\n",
    "# Load test data and extract features using custom vectorizer\n",
    "test_data, test_labels = get_lists(TEST_FILE)\n",
    "custom_features, vocab_c = get_tfidf_vectors(test_data, vocabulary=vocb)\n",
    "\n",
    "# Predict using custom features and evaluate accuracy\n",
    "predictions_custom = logisticReg_model_custom.predict(custom_features)\n",
    "average_accuracy_custom = accuracy_score(test_labels, predictions_custom)\n",
    "print(f\"Average Accuracy using features extracted from custom vectorizer: {round(average_accuracy_custom * 100, 3)}%\")\n",
    "f1_custom = f1_score(test_labels, predictions_custom)\n",
    "print(f\"F1 Score using features extracted from custom vectorizer: {round(f1_custom * 100, 3)}%\")\n",
    "\n",
    "# Train Logistic Regression model using sklearn's feature vectors\n",
    "logisticReg_model_sk = LogisticRegression()\n",
    "logisticReg_model_sk.fit(X_train_sk, y_train)\n",
    "\n",
    "# Extract features using sklearn's Tfidfvectorizer\n",
    "sk_features, vocab_sk = get_tfidf_vectors_sk(test_data, vocabulary=vocb_sk)\n",
    "\n",
    "# Predict using sklearn's features and evaluate accuracy\n",
    "predictions_sk = logisticReg_model_sk.predict(sk_features)\n",
    "average_accuracy_sk = accuracy_score(test_labels, predictions_sk)\n",
    "print(f\"Average Accuracy using features extracted from sklearn's Tfidfvectorizer: {round(average_accuracy_sk * 100, 3)}%\")\n",
    "f1_sk = f1_score(test_labels, predictions_sk)\n",
    "print(f\"F1 Score using features extracted from sklearn's Tfidfvectorizer: {round(f1_sk * 100, 3)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5679888",
   "metadata": {},
   "source": [
    "NOTE: we're expecting to see a F1 score of around 80% using both your custom features and the sklearn features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0220a93-ba22-411a-962f-983ffe3a34f2",
   "metadata": {
    "id": "f0220a93-ba22-411a-962f-983ffe3a34f2"
   },
   "source": [
    "Finally, repeat the process (training and testing), but this time, set the max_features argument to 1000 for both our custom vectorizer and sklearn's Tfidfvectorizer. Report average accuracy and F1 scores for both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f995a09c-0447-422d-8b37-e9120cfadfab",
   "metadata": {
    "id": "f995a09c-0447-422d-8b37-e9120cfadfab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Average Accuracy using features extracted from custom vectorizer with max features = 1000: 52.5%\n",
      "F1 Score using features extracted from custom vectorizer with max features = 1000: 56.621%\n",
      "Average Accuracy using features extracted from sklearn's Tfidfvectorizer with max features = 1000: 57.0%\n",
      "F1 Score using features extracted from sklearn's Tfidfvectorizer with max features = 1000: 59.048%\n",
      "Average Accuracy using features extracted from custom vectorizer: 80.0%\n",
      "F1 Score using features extracted from custom vectorizer: 80.769%\n",
      "Average Accuracy using features extracted from sklearn's Tfidfvectorizer: 81.5%\n",
      "F1 Score using features extracted from sklearn's Tfidfvectorizer: 82.629%\n"
     ]
    }
   ],
   "source": [
    "###### YOUR CODE HERE #######\n",
    "\n",
    "# First use sklearn's LogisticRegression classifier to do sentiment analysis using your custom feature vectors:\n",
    "\n",
    "###### YOUR CODE HERE #######\n",
    "X_train_c, vocab_c = get_tfidf_vectors(train_corpus, max_features=1000)\n",
    "print(len(vocab_c))\n",
    "logisticReg_model_custom = LogisticRegression()\n",
    "logisticReg_model_custom.fit(X_train_c, y_train)\n",
    "\n",
    "# Load the test data, extract features using your custom vectorizer, and test the performance of the LR classifier\n",
    "\n",
    "###### YOUR CODE HERE #######\n",
    "test_data, test_labels = get_lists(TEST_FILE)\n",
    "\n",
    "custom_X_test, vocab_c_notuse = get_tfidf_vectors(test_data, max_features=1000)#vocabulary=vocab_c)\n",
    "predictions = logisticReg_model_custom.predict(custom_X_test)\n",
    "\n",
    "\n",
    "# Print the accuracy of your model on the test data\n",
    "\n",
    "###### YOUR CODE HERE #######\n",
    "average_accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Average Accuracy using features extracted from custom vectorizer with max features = 1000: {round(average_accuracy*100,3)}%\")\n",
    "\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "print(f\"F1 Score using features extracted from custom vectorizer with max features = 1000: {round(f1*100,3)}%\")\n",
    "\n",
    "# Now repeat the above steps, but this time using features extracted by sklearn's Tfidfvectorizer\n",
    "\n",
    "###### YOUR CODE HERE #######\n",
    "X_train_sk, vocab_sk = get_tfidf_vectors_sk(train_corpus, max_features=1000)\n",
    "logisticReg_model_sk = LogisticRegression()\n",
    "logisticReg_model_sk.fit(X_train_sk, y_train)\n",
    "\n",
    "sk_X_test, vocab_sk_notuse = get_tfidf_vectors_sk(test_data, max_features=1000)#vocabulary=vocab_sk)\n",
    "predictions_sk = logisticReg_model_sk.predict(sk_X_test)\n",
    "\n",
    "average_accuracy_sk = accuracy_score(test_labels, predictions_sk)\n",
    "print(f\"Average Accuracy using features extracted from sklearn's Tfidfvectorizer with max features = 1000: {round(average_accuracy_sk*100,3)}%\")\n",
    "\n",
    "f1_sk = f1_score(test_labels, predictions_sk)\n",
    "print(f\"F1 Score using features extracted from sklearn's Tfidfvectorizer with max features = 1000: {round(f1_sk*100,3)}%\")\n",
    "\n",
    "# Second part without max_features\n",
    "\n",
    "###### YOUR CODE HERE #######\n",
    "# First use sklearn's LogisticRegression classifier to do sentiment analysis using your custom feature vectors:\n",
    "\n",
    "###### YOUR CODE HERE #######\n",
    "X_train_c, vocab_c = get_tfidf_vectors(train_corpus)\n",
    "logisticReg_model_custom = LogisticRegression()\n",
    "logisticReg_model_custom.fit(X_train_c, y_train)\n",
    "\n",
    "# Load the test data, extract features using your custom vectorizer, and test the performance of the LR classifier\n",
    "\n",
    "###### YOUR CODE HERE #######\n",
    "test_data, test_labels = get_lists(TEST_FILE)\n",
    "\n",
    "custom_X_test, vocab_c_notuse = get_tfidf_vectors(test_data, vocabulary=vocab_c)\n",
    "predictions = logisticReg_model_custom.predict(custom_X_test)\n",
    "\n",
    "\n",
    "# Print the accuracy of your model on the test data\n",
    "\n",
    "###### YOUR CODE HERE #######\n",
    "average_accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Average Accuracy using features extracted from custom vectorizer: {round(average_accuracy*100,3)}%\")\n",
    "\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "print(f\"F1 Score using features extracted from custom vectorizer: {round(f1*100,3)}%\")\n",
    "\n",
    "# Now repeat the above steps, but this time using features extracted by sklearn's Tfidfvectorizer\n",
    "\n",
    "###### YOUR CODE HERE #######\n",
    "X_train_sk, vocab_sk = get_tfidf_vectors_sk(train_corpus)\n",
    "logisticReg_model_sk = LogisticRegression()\n",
    "logisticReg_model_sk.fit(X_train_sk, y_train)\n",
    "\n",
    "sk_X_test, vocab_sk_notuse = get_tfidf_vectors_sk(test_data, vocabulary= vocab_sk)\n",
    "predictions_sk = logisticReg_model_sk.predict(sk_X_test)\n",
    "\n",
    "average_accuracy_sk = accuracy_score(test_labels, predictions_sk)\n",
    "print(f\"Average Accuracy using features extracted from sklearn's Tfidfvectorizer: {round(average_accuracy_sk*100,3)}%\")\n",
    "\n",
    "f1_sk = f1_score(test_labels, predictions_sk)\n",
    "print(f\"F1 Score using features extracted from sklearn's Tfidfvectorizer: {round(f1_sk*100,3)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2858e409-8a87-43aa-8646-1e5419cce6db",
   "metadata": {
    "id": "2858e409-8a87-43aa-8646-1e5419cce6db"
   },
   "source": [
    "1. Is there a stark difference between the two vectorizers with 1000 features?\n",
    "    - Yes, there's a notable distinction in the F1 scores. The custom vectorizer achieved an accuracy of 52.5% and an F1 score of 56.6%, while sklearn's TfidfVectorizer yielded an accuracy of 57% and an F1 score of 59%.\n",
    "2. Use sklearn's documentation for the Tfidfvectorizer to figure out what may be causing the performance difference (or lack thereof).\n",
    "    - Possible factors could include the loss of critical features due to restricting to the top 1000 tokens, or an abundance of high-frequency yet low-importance words that contribute minimally to the overall meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df8d77",
   "metadata": {},
   "source": [
    "NOTE: Irrespective of your conclusions, both implementations should be above 60% F1 Score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c652f8ab-3893-4021-b33f-c466b2e1d98a",
   "metadata": {
    "id": "c652f8ab-3893-4021-b33f-c466b2e1d98a"
   },
   "source": [
    "Task 3: Train a Feedforward Neural Network Model (25 points)\n",
    "----\n",
    "1. Using PyTorch, implement a feedforward neural network to do sentiment analysis. This model should take sparse vectors of length 10000 as input (note this is 10000, not 1000), and have a single output with the sigmoid activation function. The number of hidden layers, and intermediate activation choices are up to you, but please make sure your model does not take more than ~1 minute to train.\n",
    "2. Evaluate the model using PyTorch functions for average accuracy, area under the ROC curve and F1 scores (see [torcheval](https://pytorch.org/torcheval/stable/)) using both vectorizers, with max_features set to 10000 in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f563ec2-d02a-4176-908c-011acd8851de",
   "metadata": {
    "id": "3f563ec2-d02a-4176-908c-011acd8851de"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# if torch.backends.mps.is_available():\n",
    "# \tdevice = torch.device(\"mps\")\n",
    "if torch.cuda.is_available():\n",
    "\tdevice = torch.device(\"cuda\")\n",
    "else:\n",
    "\tdevice = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcb6aa45-e33a-4d08-bff8-996e1f80dad2",
   "metadata": {
    "id": "fcb6aa45-e33a-4d08-bff8-996e1f80dad2"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(FeedForward, self).__init__()\n",
    "        layers = []\n",
    "        previous_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(previous_size, hidden_size))\n",
    "            layers.append(nn.ReLU())  \n",
    "            previous_size = hidden_size\n",
    "        layers.append(nn.Linear(previous_size, output_size))\n",
    "        layers.append(nn.Sigmoid())  \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba08e134-2a81-48b9-89c3-43046be4cc1b",
   "metadata": {
    "id": "ba08e134-2a81-48b9-89c3-43046be4cc1b"
   },
   "outputs": [],
   "source": [
    "# Load the data using custom and sklearn vectors\n",
    "\n",
    "###### YOUR CODE HERE #######\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def create_data_loader(X, y, batch_size=64):\n",
    "    tensor_x = torch.Tensor(X)  \n",
    "    tensor_y = torch.Tensor(y).unsqueeze(1)\n",
    "    dataset = TensorDataset(tensor_x, tensor_y)  \n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "batch_size = 32 #tried different batch sizes like 64, 128, 256\n",
    "train_loader_custom = create_data_loader(X_train_c, y_train, batch_size)\n",
    "test_loader_custom = create_data_loader(custom_X_test, test_labels, batch_size)\n",
    "\n",
    "train_loader_sklearn = create_data_loader(X_train_sk.toarray(), y_train, batch_size)\n",
    "test_loader_sklearn = create_data_loader(sk_X_test.toarray(), test_labels, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "055e1e59-4d96-4a6b-8e16-cd14fcc4b760",
   "metadata": {
    "id": "055e1e59-4d96-4a6b-8e16-cd14fcc4b760"
   },
   "outputs": [],
   "source": [
    "# Create a feedforward neural network model\n",
    "# you may use any activation function on the hidden layers\n",
    "# you should use binary cross-entropy as your loss function\n",
    "# Adam is an appropriate optimizer for this task\n",
    "\n",
    "\n",
    "###### YOUR CODE HERE #######\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "input_size_custom = X_train_c.shape[1]\n",
    "input_size_sklearn = X_train_sk.shape[1]\n",
    "\n",
    "model_custom = FeedForward(input_size_custom, [512, 256], 1).to(device)\n",
    "model_sklearn = FeedForward(input_size_sklearn, [512, 256], 1).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_custom = torch.optim.Adam(model_custom.parameters(), lr=0.001) #tried changing learning rate\n",
    "optimizer_sklearn = torch.optim.Adam(model_sklearn.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "237cbc15-3473-427d-8d3f-af1059f736b4",
   "metadata": {
    "id": "237cbc15-3473-427d-8d3f-af1059f736b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Custom Vectors:\n",
      "Epoch [10/50], Loss: 0.0000\n",
      "Epoch [20/50], Loss: 0.0000\n",
      "Epoch [30/50], Loss: 0.0000\n",
      "Epoch [40/50], Loss: 0.0000\n",
      "Epoch [50/50], Loss: 0.0000\n",
      "Training with Sklearn Vectors:\n",
      "Epoch [10/50], Loss: 0.0000\n",
      "Epoch [20/50], Loss: 0.0000\n",
      "Epoch [30/50], Loss: 0.0000\n",
      "Epoch [40/50], Loss: 0.0000\n",
      "Epoch [50/50], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 50 epochs on both custom and sklearn vectors\n",
    "\n",
    "\n",
    "###### YOUR CODE HERE #######\n",
    "\n",
    "def train_model(model, train_loader, optimizer, epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "print(\"Training with Custom Vectors:\")\n",
    "train_model(model_custom, train_loader_custom, optimizer_custom, 50)\n",
    "\n",
    "print(\"Training with Sklearn Vectors:\")\n",
    "train_model(model_sklearn, train_loader_sklearn, optimizer_sklearn, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdf1319f-41c0-4237-982f-b60d492fbbd7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdf1319f-41c0-4237-982f-b60d492fbbd7",
    "outputId": "187d0ec2-1f99-417e-b23d-de234adb2848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Custom Model:\n",
      "Custom Model - F1 Score: 0.8039\n",
      "Custom Model - AUROC: 0.9078\n",
      "Custom Model - Accuracy: 0.8000\n",
      "Evaluating Sklearn Model:\n",
      "Sklearn Model - F1 Score: 0.8038\n",
      "Sklearn Model - AUROC: 0.8892\n",
      "Sklearn Model - Accuracy: 0.7950\n"
     ]
    }
   ],
   "source": [
    "#!pip install torcheval\n",
    "\n",
    "# Evaluate the model using custom and sklearn vectors\n",
    "\n",
    "###### YOUR CODE HERE #######\n",
    "\n",
    "\n",
    "from torcheval.metrics.functional import binary_f1_score\n",
    "from torcheval.metrics import BinaryAUROC, BinaryAccuracy\n",
    "\n",
    "\n",
    "def evaluate_model_metrics(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = outputs.round().squeeze()  \n",
    "            y_true.extend(labels.squeeze().cpu().numpy())  \n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_scores.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
    "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
    "    y_scores = torch.tensor(y_scores, dtype=torch.float32)\n",
    "    f1 = binary_f1_score(y_pred, y_true)  \n",
    "\n",
    "\n",
    "    auroc = BinaryAUROC()\n",
    "    auroc.update(y_scores, y_true.int())\n",
    "    auroc_score = auroc.compute()\n",
    "\n",
    "    accuracy = BinaryAccuracy()\n",
    "    accuracy.update(y_pred, y_true)\n",
    "    accuracy_score = accuracy.compute()\n",
    "\n",
    "    return f1.item(), auroc_score.item(), accuracy_score.item()\n",
    "\n",
    "print(\"Evaluating Custom Model:\")\n",
    "f1_custom, auroc_custom, accuracy_custom = evaluate_model_metrics(model_custom, test_loader_custom)\n",
    "print(f\"Custom Model - F1 Score: {f1_custom:.4f}\")\n",
    "print(f\"Custom Model - AUROC: {auroc_custom:.4f}\")\n",
    "print(f\"Custom Model - Accuracy: {accuracy_custom:.4f}\")\n",
    "\n",
    "print(\"Evaluating Sklearn Model:\")\n",
    "f1_sklearn, auroc_sklearn, accuracy_sklearn = evaluate_model_metrics(model_sklearn, test_loader_sklearn)\n",
    "print(f\"Sklearn Model - F1 Score: {f1_sklearn:.4f}\")\n",
    "print(f\"Sklearn Model - AUROC: {auroc_sklearn:.4f}\")\n",
    "print(f\"Sklearn Model - Accuracy: {accuracy_sklearn:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a332b6",
   "metadata": {},
   "source": [
    "NOTE: As in the last task, we're expecting to see a F1 score of over 60% using both your custom features and the sklearn features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b269ee9",
   "metadata": {
    "id": "8b269ee9"
   },
   "source": [
    "5 points in this assignment are reserved for overall style (both for writing and for code submitted). All work submitted should be clear, easily interpretable, and checked for spelling, etc. (Re-read what you write and make sure it makes sense). Course staff are always happy to give grammatical help (but we won't pre-grade the content of your answers)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
